function showTheSingularityIsntNear() {
  document.getElementById("theSingularityIsntNear").innerHTML='<button class = "btn btn-info" onClick="hideTheSingularityIsntNear()">Hide Source</button><p>Futurists like Vernor Vinge and Ray Kurzweil have argued that the world is rapidly approaching a tipping point, where the accelerating pace of smarter and smarter machines will soon outrun all human capabilities. They call this tipping point the singularity, because they believe it is impossible to predict how the human future might unfold after this point. Once these machines exist, Kurzweil and Vinge claim, they\'ll possess a superhuman intelligence that is so incomprehensible to us that we cannot even rationally guess how our life experiences would be altered. Vinge asks us to ponder the role of humans in a world where machines are as much smarter than us as we are smarter than our pet dogs and cats. Kurzweil, who is a bit more optimistic, envisions a future in which developments in medical nanotechnology will allow us to download a copy of our individual brains into these superhuman machines, leave our bodies behind, and, in a sense, live forever. It\'s heady stuff.<br><br> While we suppose this kind of singularity might one day occur, we don\'t think it is near. In fact, we think it will be a very long time coming. Kurzweil disagrees, based on his extrapolations about the rate of relevant scientific and technical progress. He reasons that the rate of progress toward the singularity isn\'t just a progression of steadily increasing capability, but is in fact exponentially accelerating what Kurzweil calls the "Law of Accelerating Returns." He writes that:<br><br> <blockquote>So we won\'t experience 100 years of progress in the 21st century it will be more like 20,000 years of progress (at today\'s rate). The "returns," such as chip speed and cost-effectiveness, also increase exponentially. There\'s even exponential growth in the rate of exponential growth. Within a few decades, machine intelligence will surpass human intelligence, leading to The Singularity ... [1] </blockquote><br><br>By working through a set of models and historical data, Kurzweil famously calculates that the singularity will arrive around 2045. <br><br>This prediction seems to us quite far-fetched. Of course, we are aware that the history of science and technology is littered with people who confidently assert that some event can\'t happen, only to be later proven wrong often in spectacular fashion. We acknowledge that it is possible but highly unlikely that Kurzweil will eventually be vindicated. An adult brain is a finite thing, so its basic workings can ultimately be known through sustained human effort. But if the singularity is to arrive by 2045, it will take unforeseeable and fundamentally unpredictable breakthroughs, and not because the Law of Accelerating Returns made it the inevitable result of a specific exponential rate of progress. <br><br>Kurzweil\'s reasoning rests on the Law of Accelerating Returns and its siblings, but these are not physical laws. They are assertions about how past rates of scientific and technical progress can predict the future rate. Therefore, like other attempts to forecast the future from the past, these "laws" will work until they don\'t. More problematically for the singularity, these kinds of extrapolations derive much of their overall exponential shape from supposing that there will be a constant supply of increasingly more powerful computing capabilities. For the Law to apply and the singularity to occur circa 2045, the advances in capability have to occur not only in a computer\'s hardware technologies (memory, processing power, bus speed, etc.) but also in the software we create to run on these more capable computers. To achieve the singularity, it isn\'t enough to just run today\'s software faster. We would also need to build smarter and more capable software programs. Creating this kind of advanced software requires a prior scientific understanding of the foundations of human cognition, and we are just scraping the surface of this. <br><br>This prior need to understand the basic science of cognition is where the "singularity is near" arguments fail to persuade us. It is true that computer hardware technology can develop amazingly quickly once we have a solid scientific framework and adequate economic incentives. However, creating the software for a real singularity-level computer intelligence will require fundamental scientific progress beyond where we are today. This kind of progress is very different than the Moore\'s Law-style evolution of computer hardware capabilities that inspired Kurzweil and Vinge. Building the complex software that would allow the singularity to happen requires us to first have a detailed scientific understanding of how the human brain works that we can use as an architectural guide, or else create it all de novo. This means not just knowing the physical structure of the brain, but also how the brain reacts and changes, and how billions of parallel neuron interactions can result in human consciousness and original thought. Getting this kind of comprehensive understanding of the brain is not impossible. If the singularity is going to occur on anything like Kurzweil\'s timeline, though, then we absolutely require a massive acceleration of our scientific progress in understanding every facet of the human brain. <br><br> But history tells us that the process of original scientific discovery just doesn\'t behave this way, especially in complex areas like neuroscience, nuclear fusion, or cancer research. Overall scientific progress in understanding the brain rarely resembles an orderly, inexorable march to the truth, let alone an exponentially accelerating one. Instead, scientific advances are often irregular, with unpredictable flashes of insight punctuating the slow grind-it-out lab work of creating and testing theories that can fit with experimental observations. Truly significant conceptual breakthroughs don\'t arrive when predicted, and every so often new scientific paradigms sweep through the field and cause scientists to re-evaluate portions of what they thought they had settled. We see this in neuroscience with the discovery of long-term potentiation, the columnar organization of cortical areas, and neuroplasticity. These kinds of fundamental shifts don\'t support the overall Moore\'s Law-style acceleration needed to get to the singularity on Kurzweil\'s schedule. <br><br> <strong>The Complexity Brake</strong><br><br> The foregoing points at a basic issue with how quickly a scientifically adequate account of human intelligence can be developed. We call this issue the complexity brake. As we go deeper and deeper in our understanding of natural systems, we typically find that we require more and more specialized knowledge to characterize them, and we are forced to continuously expand our scientific theories in more and more complex ways. Understanding the detailed mechanisms of human cognition is a task that is subject to this complexity brake. Just think about what is required to thoroughly understand the human brain at a micro level. The complexity of the brain is simply awesome. Every structure has been precisely shaped by millions of years of evolution to do a particular thing, whatever it might be. It is not like a computer, with billions of identical transistors in regular memory arrays that are controlled by a CPU with a few different elements. In the brain every individual structure and neural circuit has been individually refined by evolution and environmental factors. The closer we look at the brain, the greater the degree of neural variation we find. Understanding the neural structure of the human brain is getting harder as we learn more. Put another way, the more we learn, the more we realize there is to know, and the more we have to go back and revise our earlier understandings. We believe that one day this steady increase in complexity will end the brain is, after all, a finite set of neurons and operates according to physical principles. But for the foreseeable future, it is the complexity brake and arrival of powerful new theories, rather than the Law of Accelerating Returns, that will govern the pace of scientific progress required to achieve the singularity. <br><br>So, while we think a fine-grained understanding of the neural structure of the brain is ultimately achievable, it has not shown itself to be the kind of area in which we can make exponentially accelerating progress. But suppose scientists make some brilliant new advance in brain scanning technology. Singularity proponents often claim that we can achieve computer intelligence just by numerically simulating the brain "bottom up" from a detailed neural-level picture. For example, Kurzweil predicts the development of nondestructive brain scanners that will allow us to precisely take a snapshot a person\'s living brain at the subneuron level. He suggests that these scanners would most likely operate from inside the brain via millions of injectable medical nanobots. But, regardless of whether nanobot-based scanning succeeds (and we aren\'t even close to knowing if this is possible), Kurzweil essentially argues that this is the needed scientific advance that will gate the singularity: computers could exhibit human-level intelligence simply by loading the state and connectivity of each of a brain\'s neurons inside a massive digital brain simulator, hooking up inputs and outputs, and pressing "start." <br><br>However, the difficulty of building human-level software goes deeper than computationally modeling the structural connections and biology of each of our neurons. "Brain duplication" strategies like these presuppose that there is no fundamental issue in getting to human cognition other than having sufficient computer power and neuron structure maps to do the simulation.[2] While this may be true theoretically, it has not worked out that way in practice, because it doesn\'t address everything that is actually needed to build the software. For example, if we wanted to build software to simulate a bird\'s ability to fly in various conditions, simply having a complete diagram of bird anatomy isn\'t sufficient. To fully simulate the flight of an actual bird, we also need to know how everything functions together. In neuroscience, there is a parallel situation. Hundreds of attempts have been made (using many different organisms) to chain together simulations of different neurons along with their chemical environment. The uniform result of these attempts is that in order to create an adequate simulation of the real ongoing neural activity of an organism, you also need a vast amount of knowledge about the functional role that these neurons play, how their connection patterns evolve, how they are structured into groups to turn raw stimuli into information, and how neural information processing ultimately affects an organism\'s behavior. Without this information, it has proven impossible to construct effective computer-based simulation models. Especially for the cognitive neuroscience of humans, we are not close to the requisite level of functional knowledge. Brain simulation projects underway today model only a small fraction of what neurons do and lack the detail to fully simulate what occurs in a brain. The pace of research in this area, while encouraging, hardly seems to be exponential. Again, as we learn more and more about the actual complexity of how the brain functions, the main thing we find is that the problem is actually getting harder.<br><br><strong>The AI Approach</strong><br><br> Singularity proponents occasionally appeal to developments in artificial intelligence (AI) as a way to get around the slow rate of overall scientific progress in bottom-up, neuroscience-based approaches to cognition. It is true that AI has had great successes in duplicating certain isolated cognitive tasks, most recently with IBM\'s Watson system for Jeopardy! question answering. But when we step back, we can see that overall AI-based capabilities haven\'t been exponentially increasing either, at least when measured against the creation of a fully general human intelligence. While we have learned a great deal about how to build individual AI systems that do seemingly intelligent things, our systems have always remained brittle their performance boundaries are rigidly set by their internal assumptions and defining algorithms, they cannot generalize, and they frequently give nonsensical answers outside of their specific focus areas. A computer program that plays excellent chess can\'t leverage its skill to play other games. The best medical diagnosis programs contain immensely detailed knowledge of the human body but can\'t deduce that a tightrope walker would have a great sense of balance.<br><br> Why has it proven so difficult for AI researchers to build human-like intelligence, even at a small scale? One answer involves the basic scientific framework that AI researchers use. As humans grow from infants to adults, they begin by acquiring a general knowledge about the world, and then continuously augment and refine this general knowledge with specific knowledge about different areas and contexts. AI researchers have typically tried to do the opposite: they have built systems with deep knowledge of narrow areas, and tried to create a more general capability by combining these systems. This strategy has not generally been successful, although Watson\'s performance on Jeopardy! indicates paths like this may yet have promise. The few attempts that have been made to directly create a large amount of general knowledge of the world, and then add the specialized knowledge of a domain (for example, the work of Cycorp), have also met with only limited success. And in any case, AI researchers are only just beginning to theorize about how to effectively model the complex phenomena that give human cognition its unique flexibility: uncertainty, contextual sensitivity, rules of thumb, self-reflection, and the flashes of insight that are essential to higher-level thought. Just as in neuroscience, the AI-based route to achieving singularity-level computer intelligence seems to require many more discoveries, some new Nobel-quality theories, and probably even whole new research approaches that are incommensurate with what we believe now. This kind of basic scientific progress doesn\'t happen on a reliable exponential growth curve. So although developments in AI might ultimately end up being the route to the singularity, again the complexity brake slows our rate of progress, and pushes the singularity considerably into the future.<br><br> The amazing intricacy of human cognition should serve as a caution to those who claim the singularity is close. Without having a scientifically deep understanding of cognition, we can\'t create the software that could spark the singularity. Rather than the ever-accelerating advancement predicted by Kurzweil, we believe that progress toward this understanding is fundamentally slowed by the complexity brake. Our ability to achieve this understanding, via either the AI or the neuroscience approaches, is itself a human cognitive act, arising from the unpredictable nature of human ingenuity and discovery. Progress here is deeply affected by the ways in which our brains absorb and process new information, and by the creativity of researchers in dreaming up new theories. It is also governed by the ways that we socially organize research work in these fields, and disseminate the knowledge that results. At Vulcan and at the Allen Institute for Brain Science, we are working on advanced tools to help researchers deal with this daunting complexity, and speed them in their research. Gaining a comprehensive scientific understanding of human cognition is one of the hardest problems there is. We continue to make encouraging progress. But by the end of the century, we believe, we will still be wondering if the singularity is near.<br><br><i>Paul G. Allen, who cofounded Microsoft in 1975, is a philanthropist and chairman of Vulcan, which invests in an array of technology, aerospace, entertainment, and sports businesses. Mark Greaves is a computer scientist who serves as Vulcan\'s director for knowledge systems. </i><br><br>[1] Kurzweil, "The Law of Accelerating Returns," March 2001. <br><br>[2] We are beginning to get within range of the computer power we might need to support this kind of massive brain simulation. Petaflop-class computers (such as IBM\'s BlueGene/P that was used in the Watson system) are now available commercially. Exaflop-class computers are currently on the drawing boards. These systems could probably deploy the raw computational capability needed to simulate the firing patterns for all of a brain\'s neurons, though currently it happens many times more slowly than would happen in an actual brain.</p><button class = "btn btn-info" onClick="hideTheSingularityIsntNear()">Hide Source</button>';
  document.getElementById("divTheSingularityIsntNear").className = "col-lg-12 col-md-12 col-sm-12 col-xs-12";
}
function hideTheSingularityIsntNear() {
  document.getElementById("theSingularityIsntNear").innerHTML='<button class = "btn btn-primary" onClick="showTheSingularityIsntNear()">Show Source</button>'
  document.getElementById("divTheSingularityIsntNear").className = "col-lg-6 col-md-6 col-sm-12 col-xs-12";
}


function showDontUnderestimateTheSingularity() {
  document.getElementById("dontUnderestimateTheSingularity").innerHTML='<button class = "btn btn-info" onClick="hideDontUnderestimateTheSingularity()">Hide Source</button><p>Although Paul Allen paraphrases my 2005 book, The Singularity Is Near, in the title of his essay (cowritten with his colleague Mark Greaves), it appears that he has not actually read the book. His only citation is to an essay I wrote in 2001 ("The Law of Accelerating Returns") and his article does not acknowledge or respond to arguments I actually make in the book. <br><br>When my 1999 book, The Age of Spiritual Machines, was published, and augmented a couple of years later by the 2001 essay, it generated several lines of criticism, such as Moore\'s law will come to an end, hardware capability may be expanding exponentially but software is stuck in the mud, the brain is too complicated, there are capabilities in the brain that inherently cannot be replicated in software, and several others. I specifically wrote The Singularity Is Near to respond to those critiques. <br><br>I cannot say that Allen would necessarily be convinced by the arguments I make in the book, but at least he could have responded to what I actually wrote. Instead, he offers de novo arguments as if nothing has ever been written to respond to these issues. Allen\'s descriptions of my own positions appear to be drawn from my 10-year-old essay. While I continue to stand by that essay, Allen does not summarize my positions correctly even from that essay. <br><br>Allen writes that "the Law of Accelerating Returns (LOAR)... is not a physical law." I would point out that most scientific laws are not physical laws, but result from the emergent properties of a large number of events at a finer level. A classical example is the laws of thermodynamics (LOT). If you look at the mathematics underlying the LOT, they model each particle as following a random walk. So by definition, we cannot predict where any particular particle will be at any future time. Yet the overall properties of the gas are highly predictable to a high degree of precision according to the laws of thermodynamics. So it is with the law of accelerating returns. Each technology project and contributor is unpredictable, yet the overall trajectory as quantified by basic measures of price-performance and capacity nonetheless follow remarkably predictable paths. <br><br>If computer technology were being pursued by only a handful of researchers, it would indeed be unpredictable. But it\'s being pursued by a sufficiently dynamic system of competitive projects that a basic measure such as instructions per second per constant dollar follows a very smooth exponential path going back to the 1890 American census. I discuss the theoretical basis for the LOAR extensively in my book, but the strongest case is made by the extensive empirical evidence that I and others present. <br><br>Allen writes that "these \'laws\' work until they don\'t." Here, Allen is confusing paradigms with the ongoing trajectory of a basic area of information technology. If we were examining the trend of creating ever-smaller vacuum tubes, the paradigm for improving computation in the 1950s, it\'s true that this specific trend continued until it didn\'t. But as the end of this particular paradigm became clear, research pressure grew for the next paradigm. The technology of transistors kept the underlying trend of the exponential growth of price-performance going, and that led to the fifth paradigm (Moore\'s law) and the continual compression of features on integrated circuits. There have been regular predictions that Moore\'s law will come to an end. The semiconductor industry\'s roadmap titled projects seven-nanometer features by the early 2020s. At that point, key features will be the width of 35 carbon atoms, and it will be difficult to continue shrinking them. However, Intel and other chip makers are already taking the first steps toward the sixth paradigm, which is computing in three dimensions to continue exponential improvement in price performance. Intel projects that three-dimensional chips will be mainstream by the teen years. Already three-dimensional transistors and three-dimensional memory chips have been introduced. <br><br>This sixth paradigm will keep the LOAR going with regard to computer price performance to the point, later in this century, where a thousand dollars of computation will be trillions of times more powerful than the human brain. <br><br>[1] And it appears that Allen and I are at least in agreement on what level of computation is required to functionally simulate the human brain. [2] Allen then goes on to give the standard argument that software is not progressing in the same exponential manner of hardware. In The Singularity Is Near, I address this issue at length, citing different methods of measuring complexity and capability in software that demonstrate a similar exponential growth. One recent study ("Report to the President and Congress, Designing a Digital Future: Federally Funded Research and Development in Networking and Information Technology" by the President\'s Council of Advisors on Science and Technology) states the following: <br><br>"Even more remarkable and even less widely understood is that in many areas, performance gains due to improvements in algorithms have vastly exceeded even the dramatic performance gains due to increased processor speed. The algorithms that we use today for speech recognition, for natural language translation, for chess playing, for logistics planning, have evolved remarkably in the past decade ... Here is just one example, provided by Professor Martin Grotschel of Konrad-Zuse-Zentrum für Informationstechnik Berlin. Grotschel, an expert in optimization, observes that a benchmark production planning model solved using linear programming would have taken 82 years to solve in 1988, using the computers and the linear programming algorithms of the day. Fifteen years later in 2003 this same model could be solved in roughly one minute, an improvement by a factor of roughly 43 million. Of this, a factor of roughly 1,000 was due to increased processor speed, whereas a factor of roughly 43,000 was due to improvements in algorithms! Grotschel also cites an algorithmic improvement of roughly 30,000 for mixed integer programming between 1991 and 2008. The design and analysis of algorithms, and the study of the inherent computational complexity of problems, are fundamental subfields of computer science." <br><br>I cite many other examples like this in the book. [3] <br><br>Regarding AI, Allen is quick to dismiss IBM\'s Watson as narrow, rigid, and brittle. I get the sense that Allen would dismiss any demonstration short of a valid passing of the Turing test. I would point out that Watson is not so narrow. It deals with a vast range of human knowledge and is capable of dealing with subtle forms of language, including puns, similes, and metaphors. It\'s not perfect, but neither are humans, and it was good enough to get a higher score than the best two human Jeopardy! players put together. <br><br>Allen writes that Watson was put together by the scientists themselves, building each link of narrow knowledge in specific areas. Although some areas of Watson\'s knowledge were programmed directly, according to IBM, Watson acquired most of its knowledge on its own by reading natural language documents such as encyclopedias. That represents its key strength. It not only is able to understand the convoluted language in Jeopardy! queries (answers in search of a question), but it acquired its knowledge by reading vast amounts of natural-language documents. IBM is now working with Nuance (a company I originally founded as Kurzweil Computer Products) to have Watson read tens of thousands of medical articles to create a medical diagnostician. <br><br>A word on the nature of Watson\'s "understanding" is in order here. A lot has been written that Watson works through statistical knowledge rather than "true" understanding. Many readers interpret this to mean that Watson is merely gathering statistics on word sequences. The term "statistical information" in the case of Watson refers to distributed coefficients in self-organizing methods such as Markov models. One could just as easily refer to the distributed neurotransmitter concentrations in the human cortex as "statistical information." Indeed, we resolve ambiguities in much the same way that Watson does by considering the likelihood of different interpretations of a phrase. <br><br>Allen writes: "Every structure [in the brain] has been precisely shaped by millions of years of evolution to do a particular thing, whatever it might be. It is not like a computer, with billions of identical transistors in regular memory arrays that are controlled by a CPU with a few different elements. In the brain, every individual structure and neural circuit has been individually refined by evolution and environmental factors." <br><br>Allen\'s statement that every structure and neural circuit is unique is simply impossible. That would mean that the design of the brain would require hundreds of trillions of bytes of information. Yet the design of the brain (like the rest of the body) is contained in the genome. And while the translation of the genome into a brain is not straightforward, the brain cannot have more design information than the genome. Note that epigenetic information (such as the peptides controlling gene expression) do not appreciably add to the amount of information in the genome. Experience and learning do add significantly to the amount of information, but the same can be said of AI systems. I show in The Singularity Is Near that after lossless compression (due to massive redundancy in the genome), the amount of design information in the genome is about 50 million bytes, roughly half of which pertains to the brain. [4] That\'s not simple, but it is a level of complexity we can deal with and represents less complexity than many software systems in the modern world. <br><br>How do we get on the order of 100 trillion connections in the brain from only tens of millions of bytes of design information? Obviously, the answer is through redundancy. There are on the order of a billion pattern-recognition mechanisms in the cortex. They are interconnected in intricate ways, but even in the connections there is massive redundancy. The cerebellum also has billions of repeated patterns of neurons. It is true that the massively repeated structures in the brain learn different items of information as we learn and gain experience, but the same thing is true of artificially intelligent systems such as Watson. <br><br>Dharmendra S. Modha, manager of cognitive computing for IBM Research, writes: "…neuroanatomists have not found a hopelessly tangled, arbitrarily connected network, completely idiosyncratic to the brain of each individual, but instead a great deal of repeating structure within an individual brain and a great deal of homology across species ... The astonishing natural reconfigurability gives hope that the core algorithms of neurocomputation are independent of the specific sensory or motor modalities and that much of the observed variation in cortical structure across areas represents a refinement of a canonical circuit; it is indeed this canonical circuit we wish to reverse engineer." <br><br>Allen articulates what I describe in my book as the "scientist\'s pessimism." Scientists working on the next generation are invariably struggling with that next set of challenges, so if someone describes what the technology will look like in 10 generations, their eyes glaze over. One of the pioneers of integrated circuits was describing to me recently the struggles to go from 10 micron (10,000-nanometer) feature sizes to five-micron (5,000 nanometers) features over 30 years ago. They were cautiously confident of this goal, but when people predicted that someday we would actually have circuitry with feature sizes under one micron (1,000 nanometers), most of the scientists struggling to get to five microns thought that was too wild to contemplate. Objections were made on the fragility of circuitry at that level of precision, thermal effects, and so on. Well, today, Intel is starting to use chips with 22-nanometer gate lengths. <br><br>We saw the same pessimism with the genome project. Halfway through the 15-year project, only 1 percent of the genome had been collected, and critics were proposing basic limits on how quickly the genome could be sequenced without destroying the delicate genetic structures. But the exponential growth in both capacity and price performance continued (both roughly doubling every year), and the project was finished seven years later. The project to reverse-engineer the human brain is making similar progress. It is only recently, for example, that we have reached a threshold with noninvasive scanning techniques that we can see individual interneuronal connections forming and firing in real time. <br><br>Allen\'s "complexity brake" confuses the forest with the trees. If you want to understand, model, simulate, and re-create a pancreas, you don\'t need to re-create or simulate every organelle in every pancreatic Islet cell. You would want, instead, to fully understand one Islet cell, then abstract its basic functionality, and then extend that to a large group of such cells. This algorithm is well understood with regard to Islet cells. There are now artificial pancreases that utilize this functional model being tested. Although there is certainly far more intricacy and variation in the brain than in the massively repeated Islet cells of the pancreas, there is nonetheless massive repetition of functions. <br><br>Allen mischaracterizes my proposal to learn about the brain from scanning the brain to understand its fine structure. It is not my proposal to simulate an entire brain "bottom up" without understanding the information processing functions. We do need to understand in detail how individual types of neurons work, and then gather information about how functional modules are connected. The functional methods that are derived from this type of analysis can then guide the development of intelligent systems. Basically, we are looking for biologically inspired methods that can accelerate work in AI, much of which has progressed without significant insight as to how the brain performs similar functions. From my own work in speech recognition, I know that our work was greatly accelerated when we gained insights as to how the brain prepares and transforms auditory information. <br><br>The way that these massively redundant structures in the brain differentiate is through learning and experience. The current state of the art in AI does, however, enable systems to also learn from their own experience. The Google self-driving cars (which have driven over 140,000 miles through California cities and towns) learn from their own driving experience as well as from Google cars driven by human drivers. As I mentioned, Watson learned most of its knowledge by reading on its own. <br><br>It is true that Watson is not quite at human levels in its ability to understand human language (if it were, we would be at the Turing test level now), yet it was able to defeat the best humans. This is because of the inherent speed and reliability of memory that computers have. So when a computer does reach human levels, which I believe will happen by the end of the 2020s, it will be able to go out on the Web and read billions of pages as well as have experiences in online virtual worlds. Combining human-level pattern recognition with the inherent speed and accuracy of computers will be very powerful. But this is not an alien invasion of intelligence machines we create these tools to make ourselves smarter. I think Allen will agree with me that this is what is unique about the human species: we build these tools to extend our own reach. <br><br<i>Ray Kurzweil is an inventor and author. His last piece for Technology Review was about fighting the aging process.</i><br><br> [1] Chapter 2, The Singularity Is Near by Ray Kurzweil, Viking, 2005. <br><br>[2] See Endnote 2 in "The Singularity Isn\'t Near" by Paul G. Allen and Mark Greaves. <br><br>[3] Chapter 9, The Singularity Is Near. <br><br>[4] Chapter 4, The Singularity Is Near.</p><button class = "btn btn-info" onClick="hideDontUnderestimateTheSingularity()">Hide Source</button>'  ;
  document.getElementById("divDontUnderestimateTheSingularity").className = "col-lg-12 col-md-12 col-sm-12 col-xs-12";
}
function hideDontUnderestimateTheSingularity() {
  document.getElementById("dontUnderestimateTheSingularity").innerHTML='<button class = "btn btn-primary" onClick="showDontUnderestimateTheSingularity()">Show Source</button>';
  document.getElementById("divDontUnderestimateTheSingularity").className = "col-lg-6 col-md-6 col-sm-12 col-xs-12";
}


function showShouldWeFearVideo() {
  document.getElementById("shouldWeFearVideo").innerHTML='<iframe width="640" height="480" src="https://www.youtube.com/embed/TcX_7SVI_hA" frameborder="0" allowfullscreen></iframe><button class = "btn btn-info" onClick="hideShouldWeFearVideo()">Hide Source</button>';
  document.getElementById("divShouldWeFearVideo").className = "col-lg-12 col-md-12 col-sm-12 col-xs-12";
}
function hideShouldWeFearVideo() {
  document.getElementById("shouldWeFearVideo").innerHTML='<button class = "btn btn-primary" onClick="showShouldWeFearVideo()">Show Source</button>';
  document.getElementById("divShouldWeFearVideo").className = "col-lg-12 col-md-12 col-sm-12 col-xs-12";
}


function showComingSingularity() {
  document.getElementById("comingSingularity").innerHTML='<button class = "btn btn-info" onClick="hideComingSingularity()">Hide Source</button><p>The Coming Technological Singularity: How to Survive in the Post-Human Era Vernor Vinge Department of Mathematical Sciences San Diego State University (c) 1993 by Vernor Vinge (Verbatim copying/translation and distribution of this entire article is permitted in any medium, provided this notice is preserved.) This article was for the VISION-21 Symposium sponsored by NASA Lewis Research Center and the Ohio Aerospace Institute, March 30-31, 1993. It is also retrievable from the NASA technical reports server as part of NASA CP-10129. A slightly changed version appeared in the Winter 1993 issue of _Whole Earth Review_. <br><br>Abstract<br><br>Within thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended. <br>Is such progress avoidable? If not to be avoided, can events be guided so that we may survive? These questions are investigated. Some possible answers (and some further dangers) are presented. <br><br><u>What is The Singularity?</u><br><br> The acceleration of technological progress has been the central feature of this century. I argue in this paper that we are on the edge of change comparable to the rise of human life on Earth. The precise cause of this change is the imminent creation by technology of entities with greater than human intelligence. There are several means by which science may achieve this breakthrough (and this is another reason for having confidence that the event will occur): <br><ul><li>The development of computers that are "awake" and superhumanly intelligent. (To date, most controversy in the area of AI relates to whether we can create human equivalence in a machine. But if the answer is "yes, we can", then there is little doubt that beings more intelligent can be constructed shortly thereafter.</li><li>Large computer networks (and their associated users) may "wake up" as a superhumanly intelligent entity.</li><li>Computer/human interfaces may become so intimate that users may reasonably be considered superhumanly intelligent.</li><li>Biological science may find ways to improve upon the natural human intellect.</li></ul><br><br>The first three possibilities depend in large part on improvements in computer hardware. Progress in computer hardware has followed an amazingly steady curve in the last few decades [16]. Based largely on this trend, I believe that the creation of greater than human intelligence will occur during the next thirty years. (Charles Platt [19] has pointed out the AI enthusiasts have been making claims like this for the last thirty years. Just so I\'m not guilty of a relative-time ambiguity, let me more specific: I\'ll be surprised if this event occurs before 2005 or after 2030.) <br><br>What are the consequences of this event? When greater-than-human intelligence drives progress, that progress will be much more rapid. In fact, there seems no reason why progress itself would not involve the creation of still more intelligent entities -- on a still-shorter time scale. The best analogy that I see is with the evolutionary past: Animals can adapt to problems and make inventions, but often no faster than natural selection can do its work -- the world acts as its own simulator in the case of natural selection. We humans have the ability to internalize the world and conduct "what if\'s" in our heads; we can solve many problems thousands of times faster than natural selection. Now, by creating the means to execute those simulations at much higher speeds, we are entering a regime as radically different from our human past as we humans are from the lower animals. <br><br>From the human point of view this change will be a throwing away of all the previous rules, perhaps in the blink of an eye, an exponential runaway beyond any hope of control. Developments that before were thought might only happen in "a million years" (if ever) will likely happen in the next century. (In [4], Greg Bear paints a picture of the major changes happening in a matter of hours.) <br><br>I think it\'s fair to call this event a singularity ("the Singularity" for the purposes of this paper). It is a point where our models must be discarded and a new reality rules. As we move closer and closer to this point, it will loom vaster and vaster over human affairs till the notion becomes a commonplace. Yet when it finally happens it may still be a great surprise and a greater unknown. In the 1950s there were very few who saw it: Stan Ulam [27] paraphrased John von Neumann as saying: <br><br><blockquote>One conversation centered on the ever accelerating progress of technology and changes in the mode of human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue. </blockquote>Von Neumann even uses the term singularity, though it appears he is still thinking of normal progress, not the creation of superhuman intellect. (For me, the superhumanity is the essence of the Singularity. Without that we would get a glut of technical riches, never properly absorbed (see [24]).) <br><br>In the 1960s there was recognition of some of the implications of superhuman intelligence. I. J. Good wrote [10]: <blockquote>Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an "intelligence explosion," and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the _last_ invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.<br>...<br>It is more probable than not that, within the twentieth century, an ultraintelligent machine will be built and that it will be the last invention that man need make.</blockquote>Good has captured the essence of the runaway, but does not pursue its most disturbing consequences. Any intelligent machine of the sort he describes would not be humankind\'s "tool" -- any more than humans are the tools of rabbits or robins or chimpanzees.<br><br>Through the \'60s and \'70s and \'80s, recognition of the cataclysm spread [28] [1] [30] [4]. Perhaps it was the science-fiction writers who felt the first concrete impact. After all, the "hard" science-fiction writers are the ones who try to write specific stories about all that technology may do for us. More and more, these writers felt an opaque wall across the future. Once, they could put such fantasies millions of years in the future [23]. Now they saw that their most diligent extrapolations resulted in the unknowable ... soon. Once, galactic empires might have seemed a Post-Human domain. Now, sadly, even interplanetary ones are.<br><br>What about the \'90s and the \'00s and the \'10s, as we slide toward the edge? How will the approach of the Singularity spread across the human world view? For a while yet, the general critics of machine sapience will have good press. After all, till we have hardware as powerful as a human brain it is probably foolish to think we\'ll be able to create human equivalent (or greater) intelligence. (There is the far-fetched possibility that we could make a human equivalent out of less powerful hardware, if were willing to give up speed, if we were willing to settle for an artificial being who was literally slow [29]. But it\'s much more likely that devising the software will be a tricky process, involving lots of false starts and experimentation. If so, then the arrival of self-aware machines will not happen till after the development of hardware that is substantially more powerful than humans\' natural equipment.)<br><br>But as time passes, we should see more symptoms. The dilemma felt by science fiction writers will be perceived in other creative endeavors. (I have heard thoughtful comic book writers worry about how to have spectacular effects when everything visible can be produced by the technically commonplace.) We will see automation replacing higher and higher level jobs. We have tools right now (symbolic math programs, cad/cam) that release us from most low-level drudgery. Or put another way: The work that is truly productive is the domain of a steadily smaller and more elite fraction of humanity. In the coming of the Singularity, we are seeing the predictions of _true_ technological unemployment finally come true.<br><br>Another symptom of progress toward the Singularity: ideas themselves should spread ever faster, and even the most radical will quickly become commonplace. When I began writing, it seemed very easy to come up with ideas that took decades to percolate into the cultural consciousness; now the lead time seems more like eighteen months. (Of course, this could just be me losing my imagination as I get old, but I see the effect in others too.) Like the shock in a compressible flow, the Singularity moves closer as we accelerate through the critical speed.<br><br>And what of the arrival of the Singularity itself? What can be said of its actual appearance? Since it involves an intellectual runaway, it will probably occur faster than any technical revolution seen so far. The precipitating event will likely be unexpected -- perhaps even to the researchers involved. ("But all our previous models were catatonic! We were just tweaking some parameters....") If networking is widespread enough (into ubiquitous embedded systems), it may seem as if our artifacts as a whole had suddenly wakened.<br><br>And what happens a month or two (or a day or two) after that? I have only analogies to point to: The rise of humankind. We will be in the Post-Human era. And for all my rampant technological optimism, sometimes I think I\'d be more comfortable if I were regarding these transcendental events from one thousand years remove ... instead of twenty.<br><br><u>Can the Singularity be Avoided?</u><br><br>Well, maybe it won\'t happen at all: Sometimes I try to imagine the symptoms that we should expect to see if the Singularity is not to develop. There are the widely respected arguments of Penrose [18] and Searle [21] against the practicality of machine sapience. In August of 1992, Thinking Machines Corporation held a workshop to investigate the question "How We Will Build a Machine that Thinks" [Thearling]. As you might guess from the workshop\'s title, the participants were not especially supportive of the arguments against machine intelligence. In fact, there was general agreement that minds can exist on nonbiological substrates and that algorithms are of central importance to the existence of minds. However, there was much debate about the raw hardware power that is present in organic brains. A minority felt that the largest 1992 computers were within three orders of magnitude of the power of the human brain. The majority of the participants agreed with Moravec\'s estimate [16] that we are ten to forty years away from hardware parity. And yet there was another minority who pointed to [6] [20], and conjectured that the computational competence of single neurons may be far higher than generally believed. If so, our present computer hardware might be as much as _ten_ orders of magnitude short of the equipment we carry around in our heads. If this is true (or for that matter, if the Penrose or Searle critique is valid), we might never see a Singularity. Instead, in the early \'00s we would find our hardware performance curves begin to level off -- this caused by our inability to automate the complexity of the design work necessary to support the hardware trend curves. We\'d end up with some _very_ powerful hardware, but without the ability to push it further. Commercial digital signal processing might be awesome, giving an analog appearance even to digital operations, but nothing would ever "wake up" and there would never be the intellectual runaway which is the essence of the Singularity. It would likely be seen as a golden age ... and it would also be an end of progress. This is very like the future predicted by Gunther Stent. In fact, on page 137 of [24], Stent explicitly cites the development of transhuman intelligence as a sufficient condition to break his projections.<br><br>But if the technological Singularity can happen, it will. Even if all the governments of the world were to understand the "threat" and be in deadly fear of it, progress toward the goal would continue. In fiction, there have been stories of laws passed forbidding the construction of "a machine in the form of the mind of man" [12]. In fact, the competitive advantage -- economic, military, even artistic -- of every advance in automation is so compelling that passing laws, or having customs, that forbid such things merely assures that someone else will get them first.<br><br>Eric Drexler [7] has provided spectacular insight about how far technical improvement may go. He agrees that superhuman intelligences will be available in the near future -- and that such entities pose a threat to the human status quo. But Drexler argues that we can embed such transhuman devices in rules or physical confinement such that their results can be examined and used safely. This is I. J. Good\'s ultraintelligent machine, with a dose of caution. I argue that confinement is intrinsically impractical. For the case of physical confinement: Imagine yourself confined to your house with only limited data access to the outside, to your masters. If those masters thought at a rate -- say -- one million times slower than you, there is little doubt that over a period of years (your time) you could come up with "helpful advice" that would incidentally set you free. (I call this "fast thinking" form of superintelligence "weak superhumanity". Such a "weakly superhuman" entity would probably burn out in a few weeks of outside time. "Strong superhumanity" would be more than cranking up the clock speed on a human-equivalent mind. It\'s hard to say precisely what "strong superhumanity" would be like, but the difference appears to be profound. Imagine running a dog mind at very high speed. Would a thousand years of doggy living add up to any human insight? (Now if the dog mind were cleverly rewired and _then_ run at high speed, we might see something different....) Most speculations about superintelligence seem to be based on the weakly superhuman model. I believe that our best guesses about the post-Singularity world can be obtained by thinking on the nature of strong superhumanity. I will return to this point later in the paper.)<br><br>The other approach to Drexlerian confinement is to build _rules_ into the mind of the created superhuman entity (Asimov\'s Laws). I think that performance rules strict enough to be safe would also produce a device whose ability was clearly inferior to the unfettered versions (and so human competition would favor the development of the those more dangerous models). Still, the Asimov dream is a wonderful one: Imagine a willing slave, who has 1000 times your capabilities in every way. Imagine a creature who could satisfy your every safe wish (whatever that means) and still have 99.9% of its time free for other activities. There would be a new universe we never really understood, but filled with benevolent gods (though one of <u>my</u> wishes might be to become one of them).<br><br>If the Singularity can not be prevented or confined, just how bad could the Post-Human era be? Well ... pretty bad. The physical extinction of the human race is one possibility. (Or as Eric Drexler put it of nanotechnology: Given all that such technology can do, perhaps governments would simply decide that they no longer need citizens!). Yet physical extinction may not be the scariest possibility. Again, analogies: Think of the different ways we relate to animals. Some of the crude physical abuses are implausible, yet.... In a Post-Human world there would still be plenty of niches where human equivalent automation would be desirable: embedded systems in autonomous devices, self-aware daemons in the lower functioning of larger sentients. (A strongly superhuman intelligence would likely be a Society of Mind [15] with some very competent components.) Some of these human equivalents might be used for nothing more than digital signal processing. They would be more like whales than humans. Others might be very human-like, yet with a one-sidedness, a _dedication_ that would put them in a mental hospital in our era. Though none of these creatures might be flesh-and-blood humans, they might be the closest things in the new enviroment to what we call human now. (I. J. Good had something to say about this, though at this late date the advice may be moot: Good [11] proposed a "Meta-Golden Rule", which might be paraphrased as "Treat your inferiors as you would be treated by your superiors." It\'s a wonderful, paradoxical idea (and most of my friends don\'t believe it) since the game-theoretic payoff is so hard to articulate. Yet if we were able to follow it, in some sense that might say something about the plausibility of such kindness in this universe.)<br><br>I have argued above that we cannot prevent the Singularity, that its coming is an inevitable consequence of the humans\' natural competitiveness and the possibilities inherent in technology. And yet ... we are the initiators. Even the largest avalanche is triggered by small things. We have the freedom to establish initial conditions, make things happen in ways that are less inimical than others. Of course (as with starting avalanches), it may not be clear what the right guiding nudge really is: <br><br><u>Other Paths to the Singularity: Intelligence Amplification</u><br><br>When people speak of creating superhumanly intelligent beings, they are usually imagining an AI project. But as I noted at the beginning of this paper, there are other paths to superhumanity. Computer networks and human-computer interfaces seem more mundane than AI, and yet they could lead to the Singularity. I call this contrasting approach Intelligence Amplification (IA). IA is something that is proceeding very naturally, in most cases not even recognized by its developers for what it is. But every time our ability to access information and to communicate it to others is improved, in some sense we have achieved an increase over natural intelligence. Even now, the team of a PhD human and good computer workstation (even an off-net workstation!) could probably max any written intelligence test in existence.<br><br>And it\'s very likely that IA is a much easier road to the achievement of superhumanity than pure AI. In humans, the hardest development problems have already been solved. Building up from within ourselves ought to be easier than figuring out first what we really are and then building machines that are all of that. And there is at least conjectural precedent for this approach. Cairns-Smith [5] has speculated that biological life may have begun as an adjunct to still more primitive life based on crystalline growth. Lynn Margulis [14] has made strong arguments for the view that mutualism is the great driving force in evolution.<br><br>Note that I am not proposing that AI research be ignored or less funded. What goes on with AI will often have applications in IA, and vice versa. I am suggesting that we recognize that in network and interface research there is something as profound (and potential wild) as Artificial Intelligence. With that insight, we may see projects that are not as directly applicable as conventional interface and network design work, but which serve to advance us toward the Singularity along the IA path.<br><br>Here are some possible projects that take on special significance, given the IA point of view:<ul><li>Human/computer team automation: Take problems that are normally considered for purely machine solution (like hill-climbing problems), and design programs and interfaces that take a advantage of humans\' intuition and available computer hardware. Considering all the bizarreness of higher dimensional hill-climbing problems (and the neat algorithms that have been devised for their solution), there could be some very interesting displays and control tools provided to the human team member.</li><li>Develop human/computer symbiosis in art: Combine the graphic generation capability of modern machines and the esthetic sensibility of humans. Of course, there has been an enormous amount of research in designing computer aids for artists, as labor saving tools. I\'m suggesting that we explicitly aim for a greater merging of competence, that we explicitly recognize the cooperative approach that is possible. Karl Sims [22] has done wonderful work in this direction.</li><li>Allow human/computer teams at chess tournaments. We already have programs that can play better than almost all humans. But how much work has been done on how this power could be used by a human, to get something even better? If such teams were allowed in at least some chess tournaments, it could have the positive effect on IA research that allowing computers in tournaments had for the corresponding niche in AI.</li><li>Develop interfaces that allow computer and network access without requiring the human to be tied to one spot, sitting in front of a computer. (This is an aspect of IA that fits so well with known economic advantages that lots of effort is already being spent on it.)</li><li>Develop more symmetrical decision support systems. A popular research/product area in recent years has been decision support systems. This is a form of IA, but may be too focussed on systems that are oracular. As much as the program giving the user information, there must be the idea of the user giving the program guidance.</li><li>Use local area nets to make human teams that really work (ie, are more effective than their component members). This is generally the area of "groupware", already a very popular commercial pursuit. The change in viewpoint here would be to regard the group activity as a combination organism. In one sense, this suggestion might be regarded as the goal of inventing a "Rules of Order" for such combination operations. For instance, group focus might be more easily maintained than in classical meetings. Expertise of individual human members could be isolated from ego issues such that the contribution of different members is focussed on the team project. And of course shared data bases could be used much more conveniently than in conventional committee operations. (Note that this suggestion is aimed at team operations rather than political meetings. In a political setting, the automation described above would simply enforce the power of the persons making the rules!)</li><li>Exploit the worldwide Internet as a combination human/machine tool. Of all the items on the list, progress in this is proceeding the fastest and may run us into the Singularity before anything else. The power and influence of even the present-day Internet is vastly underestimated. For instance, I think our contemporary computer systems would break under the weight of their own complexity if it weren\'t for the edge that the USENET "group mind" gives the system administration and support people!) The very anarchy of the worldwide net development is evidence of its potential. As connectivity and bandwidth and archive size and computer speed all increase, we are seeing something like Lynn Margulis\' [14] vision of the biosphere as data processor recapitulated, but at a million times greater speed and with millions of humanly intelligent agents (ourselves).</li></ul><br><br>The above examples illustrate research that can be done within the context of contemporary computer science departments. There are other paradigms. For example, much of the work in Artificial Intelligence and neural nets would benefit from a closer connection with biological life. Instead of simply trying to model and understand biological life with computers, research could be directed toward the creation of composite systems that rely on biological life for guidance or for the providing features we don\'t understand well enough yet to implement in hardware. A long-time dream of science-fiction has been direct brain to computer interfaces [2] [28]. In fact, there is concrete work that can be done (and has been done) in this area:<ul><li>Limb prosthetics is a topic of direct commercial applicability. Nerve to silicon transducers can be made [13]. This is an exciting, near-term step toward direct communication.</li><li>Similar direct links into brains may be feasible, if the bit rate is low: given human learning flexibility, the actual brain neuron targets might not have to be precisely selected. Even 100 bits per second would be of great use to stroke victims who would otherwise be confined to menu-driven interfaces.</li><li>Plugging in to the optic trunk has the potential for bandwidths of 1 Mbit/second or so. But for this, we need to know the fine-scale architecture of vision, and we need to place an enormous web of electrodes with exquisite precision. If we want our high bandwidth connection to be _in addition_ to what paths are already present in the brain, the problem becomes vastly more intractable. Just sticking a grid of high-bandwidth receivers into a brain certainly won\'t do it. But suppose that the high-bandwidth grid were present while the brain structure was actually setting up, as the embryo develops. That suggests:</li><li>Animal embryo experiments. I wouldn\'t expect any IA success in the first years of such research, but giving developing brains access to complex simulated neural structures might be very interesting to the people who study how the embryonic brain develops. In the long run, such experiments might produce animals with additional sense paths and interesting intellectual abilities.</li></ul>Originally, I had hoped that this discussion of IA would yield some clearly safer approaches to the Singularity. (After all, IA allows our participation in a kind of transcendance.) Alas, looking back over these IA proposals, about all I am sure of is that they should be considered, that they may give us more options. But as for safety ... well, some of the suggestions are a little scarey on their face. One of my informal reviewers pointed out that IA for individual humans creates a rather sinister elite. We humans have millions of years of evolutionary baggage that makes us regard competition in a deadly light. Much of that deadliness may not be necessary in today\'s world, one where losers take on the winners\' tricks and are coopted into the winners\' enterprises. A creature that was built _de novo_ might possibly be a much more benign entity than one with a kernel based on fang and talon. And even the egalitarian view of an Internet that wakes up along with all mankind can be viewed as a nightmare [25].<br><br>The problem is not that the Singularity represents simply the passing of humankind from center stange, but that it contradicts some of our most deeply held notions of being. I think a closer look at the notion of strong superhumanity can show why that is.<br><br><u>Strong Superhumanity and the Best We Can Ask for</u><br><br> Suppose we could tailor the Singularity. Suppose we could attain our most extravagant hopes. What then would we ask for: That humans themselves would become their own successors, that whatever injustice occurs would be tempered by our knowledge of our roots. For those who remained unaltered, the goal would be benign treatment (perhaps even giving the stay-behinds the appearance of being masters of godlike slaves). It could be a golden age that also involved progress (overleaping Stent\'s barrier). Immortality (or at least a lifetime as long as we can make the universe survive [9] [3]) would be achievable.<br><br>But in this brightest and kindest world, the philosophical problems themselves become intimidating. A mind that stays at the same capacity cannot live forever; after a few thousand years it would look more like a repeating tape loop than a person. (The most chilling picture I have seen of this is in [17].) To live indefinitely long, the mind itself must grow ... and when it becomes great enough, and looks back ... what fellow-feeling can it have with the soul that it was originally? Certainly the later being would be everything the original was, but so much vastly more. And so even for the individual, the Cairns-Smith (or Lynn Margulis) notion of new life growing incrementally out of the old must still be valid.<br><br>This "problem" about immortality comes up in much more direct ways. The notion of ego and self-awareness has been the bedrock of the hardheaded rationalism of the last few centuries. Yet now the notion of self-awareness is under attack from the Artificial Intelligence people ("self-awareness and other delusions"). Intelligence Amplification undercuts the importance of ego from another direction. The post-Singularity world will involve extremely high-bandwidth networking. A central feature of strongly superhuman entities will likely be their ability to communicate at variable bandwidths, including ones far higher than speech or written messages. What happens when pieces of ego can be copied and merged, when the size of a selfawareness can grow or shrink to fit the nature of the problems under consideration? These are essential features of strong superhumanity and the Singularity. Thinking about them, one begins to feel how essentially strange and different the Post-Human era will be -- <u>no matter how cleverly and benignly it is brought to be</u>.<br><br>From one angle, the vision fits many of our happiest dreams: a place unending, where we can truly know one another and understand the deepest mysteries. From another angle, it\'s a lot like the worst case scenario I imagined earlier in this paper.<br><br>Which is the valid viewpoint? In fact, I think the new era is simply too different to fit into the classical frame of good and evil. That frame is based on the idea of isolated, immutable minds connected by tenuous, low-bandwith links. But the post-Singularity world _does_ fit with the larger tradition of change and cooperation that started long ago (perhaps even before the rise of biological life). I think there _are_ notions of ethics that would apply in such an era. Research into IA and high-bandwidth communications should improve this understanding. I see just the glimmerings of this now, in Good\'s Meta-Golden Rule, perhaps in rules for distinguishing self from others on the basis of bandwidth of connection. And while mind and self will be vastly more labile than in the past, much of what we value (knowledge, memory, thought) need never be lost. I think Freeman Dyson has it right when he says [8]: "God is what mind becomes when it has passed beyond the scale of our comprehension."<br><br>[I wish to thank John Carroll of San Diego State University and Howard Davidson of Sun Microsystems for discussing the draft version of this paper with me.]<br><br><u>Annotated Sources [and an occasional plea for bibliographical help]</u><br><br> [1] Alfvén, Hannes, writing as Olof Johanneson, _The End of Man?_, Award Books, 1969 earlier published as "The Tale of the Big Computer", Coward-McCann, translated from a book copyright 1966 Albert Bonniers Forlag AB with English translation copyright 1966 by Victor Gollanz, Ltd.<br><br>[2] Anderson, Poul, "Kings Who Die", _If_, March 1962, p8-36. Reprinted in _Seven Conquests_, Poul Anderson, MacMillan Co., 1969.<br><br>[3] Barrow, John D. and Frank J. Tipler, _The Anthropic Cosmological Principle_, Oxford University Press, 1986.<br><br>[4] Bear, Greg, "Blood Music", _Analog Science Fiction-Science Fact_, June, 1983. Expanded into the novel _Blood Music_, Morrow, 1985<br><br>[5] Cairns-Smith, A. G., <u>Seven Clues to the Origin of Life</u>, Cambridge University Press, 1985.<br><br>[6] Conrad, Michael <u>et al.</u>, "Towards an Artificial Brain", <u>BioSystems</u>, vol23, pp175-218, 1989.<br><br>[7] Drexler, K. Eric, <u>Engines of Creation</u>, Anchor Press/Doubleday, 1986.<br><br>[8] Dyson, Freeman, <u>Infinite in All Directions</u>, Harper && Row, 1988.<br><br>[9] Dyson, Freeman, "Physics and Biology in an Open Universe", <u>Review of Modern Physics</u>, vol 51, pp447-460, 1979.<br><br>[10] Good, I. J., "Speculations Concerning the First Ultraintelligent Machine", in _Advances in Computers_, vol 6, Franz L. Alt and Morris Rubinoff, eds, pp31-88, 1965, Academic Press.<br><br>[11] Good, I. J., [Help! I can\'t find the source of Good\'s Meta-Golden Rule, though I have the clear recollection of hearing about it sometime in the 1960s. Through the help of the net, I have found pointers to a number of related items. G. Harry Stine and Andrew Haley have written about metalaw as it might relate to extraterrestrials: G. Harry Stine, "How to Get along with Extraterrestrials or Your Neighbor", _Analog Science Fact- Science Fiction_, February, 1980, p39-47.]<br><br>[12] Herbert, Frank, _Dune_, Berkley Books, 1985. However, this novel was serialized in _Analog Science Fiction-Science Fact_ in the 1960s.<br><br>[13] Kovacs, G. T. A. _et al._, "Regeneration Microelectrode Array for Peripheral Nerve Recording and Stimulation", _IEEE Transactions on Biomedical Engineering_, v 39, n 9, pp 893-902.<br><br>[14] Margulis, Lynn and Dorion Sagan, _Microcosmos, Four Billion Years of Evolution from Our Microbial Ancestors_, Summit Books, 1986.<br><br>[15] Minsky, Marvin, _Society of Mind_, Simon and Schuster, 1985.<br><br>[16] Moravec, Hans, _Mind Children_, Harvard University Press, 1988.<br><br>[17] Niven, Larry, "The Ethics of Madness", _If_, April 1967, pp82-108. Reprinted in _Neutron Star_, Larry Niven, Ballantine Books, 1968.<br><br>[18] Penrose, R., _The Emperor\'s New Mind_, Oxford University Press, 1989.<br><br>[19] Platt, Charles, Private Communication.<br><br>[20] Rasmussen, S. _et al._, "Computational Connectionism within Neurons: a Model of Cytoskeletal Automata Subserving Neural Networks", in _Emergent Computation_, Stephanie Forrest, ed., p428-449, MIT Press, 1991.<br><br>[21] Searle, John R., "Minds, Brains, and Programs", in _The Behavioral and Brain Sciences_, v.3, Cambridge University Press, 1980. The essay is reprinted in _The Mind\'s I_, edited by Douglas R. Hofstadter and Daniel C. Dennett, Basic Books, 1981. This reprinting contains an excellent critique of the Searle essay.<br><br>[22] Sims, Karl, "Interactive Evolution of Dynamical Systems", Thinking Machines Corporation, Technical Report Series (published in _Toward a Practice of Autonomous Systems: Proceedings of the First European Cnference on Artificial Life_, Paris, MIT Press, December 1991.<br><br>[23] Stapledon, Olaf, _The Starmaker_, Berkley Books, 1961 (but from the forward probably written before 1937).<br><br>[24] Stent, Gunther S., _The Coming of the Golden Age: A View of the End of Progress_, The Natural History Press, 1969.<br><br>[25] Swanwick Michael, _Vacuum Flowers_, serialized in _Isaac Asimov\'s Science Fiction Magazine_, December(?) 1986 - February 1987. Republished by Ace Books, 1988. [26] Thearling, Kurt, "How We Will Build a Machine that Thinks", a workshop at Thinking Machines Corporation. Personal Communication.<br><br>[27] Ulam, S., Tribute to John von Neumann, _Bulletin of the American Mathematical Society_, vol 64, nr 3, part 2, May, 1958, p1-49.<br><br>[28] Vinge, Vernor, "Bookworm, Run!", _Analog_, March 1966, pp8-40. Reprinted in _True Names and Other Dangers_, Vernor Vinge, Baen Books, 1987.<br><br>[29] Vinge, Vernor, "True Names", _Binary Star Number 5_, Dell, 1981. Reprinted in _True Names and Other Dangers_, Vernor Vinge, Baen Books, 1987.<br><br>[30] Vinge, Vernor, First Word, _Omni_, January 1983, p10.</p><button class = "btn btn-info" onClick="hideComingSingularity()">Hide Source</button>';
  document.getElementById("divComingSingularity").className = "col-lg-12 col-md-12 col-sm-12 col-xs-12";
}
function hideComingSingularity() {
  document.getElementById("comingSingularity").innerHTML='<button class = "btn btn-primary" onClick="showComingSingularity()">Show Source</button>';
  document.getElementById("divComingSingularity").className = "col-lg-6 col-md-6 col-sm-12 col-xs-12";
}


function showSingularityMayNever() {
  document.getElementById("singularityMayNever").innerHTML='<button class = "btn btn-info" onClick="hideSingularityMayNever()">Hide Source</button><p><strong>Introduction</strong><br><br> There is both much optimism and pessimism around artificial intelligence (AI) today. The optimists are investing millions of dollars, and even in some cases billions of dollars into AI. The pessimists, on the other hand, predict that AI will end many things: jobs, warfare, and even the human race. Both the optimists and the pessimists often appeal to the idea of a technological singularity, a point in time where machine intelligence starts to run away, and a new, more intelligent "species" starts to inhabit the earth. If the optimists are right, this will be a moment that fundamentally changes our economy and our society. If the pessimists are right, this will be a moment that also fundamentally changes our economy and our society. It is therefore very worthwhile spending some time deciding if either of them might be right. <br><br> <strong>The History of the Singularity</strong><br><br> The idea of a technological singularity can be traced back to a number of different thinkers. Following John von Neumann\'s death in 1957, Stanislaw Ulam wrote: <br><br> <blockquote>"One conversation [with John von Neumann] centered on the ever accelerating progress of technology and changes in the mode of human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue." (Ulam 1958)</blockquote><br><br>I.J. Good made a more specific prediction in 1965, calling it an "intelligence explosion" rather than a "singularity": <br><br><blockquote>"Let an ultra intelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultra intelligent machine could design even better machines; there would then unquestionably be an intelligence explosion, and the intelligence of man would be left far behind. Thus the first ultra intelligent machine is the last invention that man need ever make." (Good 1965)</blockquote><br><br>Many credit the technological singularity to the computer scientist, and science fiction author Vernor Vinge who predicted: <br><br> <blockquote>"Within thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended." (Vinge 1993)</blockquote><br><br>More recently, the idea of a technological singularity has been popularized by Ray Kurzweil (Kurzweil 2006) as well as others. Based on current trends, Kurzweil predicts the technological singularity will happen around 2045. For the purposes of this article, I suppose that the technological singularity is the point in time at which we build a machine of sufficient intelligence that is able to redesign itself to improve its intelligence, and at which its intelligence starts to grow exponentially fast, quickly exceeding human intelligence by orders of magnitude. <br><br>I start with two mathematical quibbles. The first quibble is that the technological singularity is not a mathematical singularity. The function 1/(1-t) has a mathematical singularity at t = 1. This function demonstrates hyperbolic growth. As t approaches 1, its derivative ceases to be finite and well defined. Many proponents of a technological singularity argue only for exponential growth. For example, the function 2 t demonstrates exponential growth. Such an exponential function approaches infinity more slowly, and has a finite derivative that is always well defined. The second quibble is that the idea of exponential growth in intelligence depends entirely on the scale used to measure intelligence. If we measure intelligence in logspace, exponential growth is merely linear. I will not tackle here head on what we mean by measuring the intelligence of machines (or of humans). I will simply suppose there is such a property as intelligence, that it can be measured and compared, and that the technological singularity is when this measure increases exponentially fast in an appropriate and reasonable scale. <br><br>The possibility of a technological singularity has driven several commentators to issue dire predictions about the possible impact of artificial intelligence on the human race. For instance, in December 2014, Stephen Hawking told the BBC: <br><br><blockquote>"The development of full artificial intelligence could spell the end of the human race. . . . It would take off on its own, and re-design itself at an ever increasing rate. Humans, who are limited by slow biological evolution, couldn\'t compete, and would be superseded. "</blockquote><br><br>Several other well known figures including Bill Gates, Elon Musk and Steve Wozniak have subsequently issued similar warnings. Nick Bostrom has predicted a technological singularity, and argued that this poses an existential threat to the human race (Bostrom 2014). In this article, I will explore arguments as to why a technological singularity might not be close. <br><br><strong>Some arguments against the Singularity</strong><br><br>The idea of a technological singularity has received more debate outside the mainstream AI community than within it. In part, this may be because many of the proponents for such an event have come from outside this community. The technological singularity also has become associated with some somewhat challenging ideas like life extension and transhumanism. This is unfortunate as it has distracted debate from a fundamental and important issue: will we able to develop machines that at some point are able to improve their intelligence exponentially fast and that quickly far exceed our own human intelligence? This might not seem a particularly wild idea. The field of computing has profited considerably from exponential trends. Moore\'s Law has predicted with reasonable accuracy that the number of transistors on an integrated circuit (and hence the amount of memory in a chip) will double every two years since 1975. And Koomeys law has accurately predicted that the number of computations per joule of energy dissipated will double every 19 months since the 1950s. Is it unreasonable to suppose AI will also at some point witness exponential growth? <br><br>The thesis put forwards here is that there are several strong arguments against the possibility of a technological singularity. Let me be precise. I am not predicting that AI will fail to achieve super-human intelligence. Like many of my colleagues working in AI, I predict we are just 30 or 40 years away from this event. However, I am suggesting that there will not be the run away exponential growth predicted by some. I will put forwards multiple arguments why a technological singularity is improbable. <br><br>These are not the only arguments against a technological singularity. We can, for instance, also inherit all the arguments raised against artificial intelligence itself. Hence, there are also the nine common objections considered by Alan Turing in his seminal Mind paper (Turing 1963) like machines not being conscious, or not being creative. I focus here though on arguments which go to the idea of an exponential run away in intelligence. <br><br><strong>The "Fast Thinking Dog" argument</strong><br><br>One of the arguments put forwards by proponents of the technological singularity is that silicon has a significant speed advantage over our brain\'s wetware, and this advantage doubles every two years or so according to Moore\'s Law. Unfortunately speed along does not bring increased intelligence. To adapt an idea from Vernor Vinge (Vinge 1993), a faster thinking dog is still unlikely to play chess. Steven Pinker put this argument eloquently: <br><br> <blockquote>"There is not the slightest reason to believe in a coming singularity. The fact that you can visualize a future in your imagination is not evidence that it is likely or even possible. Look at domed cities, jet-pack commuting, underwater cities, mile-high buildings, and nuclear-powered automobilesall staples of futuristic fantasies when I was a child that have never arrived. Sheer processing power is not a pixie dust that magically solves all your problems." (Pinker 2008)</blockquote><br><br>Intelligence is much more than thinking faster or longer about a problem than someone else. Of course, Moore\'s Law has certainly helped AI. We now learn off bigger data sets. We now learn quicker. Faster computers will certainly help us build artificial intelligence. But, at least for humans, intelligence depends on many other things including many years of experience and training. It is not at all clear that we can short circuit this in silicon simply by increasing the clock speed. <br><br><strong>The "Anthropcentric" argument</strong><br><br>Many descriptions of the technological singularity supposes human intelligence is some special point to pass, some sort of "tipping" point. For instance, Nick Bostrom writes: <br><br><blockquote>"Human-level artificial intelligence leads quickly to greater-than-human-level artificial intelligence. . . . The interval during which the machines and humans are roughly matched will likely be brief. Shortly thereafter, humans will be unable to compete intellectually with artificial minds." (Bostrom 2002)</blockquote><br><br>Human intelligence is one point on a wide spectrum that takes us from cockroach through mouse to human. Actually, it might be better to say it is a probability distribution rather than a single point. It is not clear in arguments like the above which level of human intelligence requires to be exceeded before run away growth kicks in. Is it some sort of average intelligence? Or the intelligence of the smartest human ever? <br><br>If there is one thing that we should have learnt from the history of science, it is that we are not as special as we would like to believe. Copernicus taught us that the universe did not revolve around the earth. Darwin taught us that we were little different from the apes. And artificial intelligence will likely teach us that human intelligence is itself nothing special. There is no reason therefore to suppose that human intelligence is some special tipping point, that once passed allows for rapid increases in intelligence. Of course, this doesn\'t preclude there being some level of intelligence which is a tipping point. <br><br>One argument put forwards by proponents of a technological singularity is that human intelligence is indeed a special point to pass because we are unique in being able to build artefacts that amplify our intellectual abilities. We are the only creatures on the planet with sufficient intelligence to design new intelligence, and this new intelligence will not be limited by the slow process of reproduction and evolution. However, this sort of argument supposes its conclusion. It assumes that human intelligence is enough to design an artificial intelligence that is the sufficiently intelligent to be the starting point for a technological singularity. In other words, it assumes we have enough intelligence to initiate the technological singularity, the very conclusion we are trying to draw. We may or may not have enough intelligence to be able to design such artificial intelligence. It is far from inevitable. Even if have enough intelligence to design superhuman artificial intelligence, this super-human artificial intelligence may not be adequate to precipitate a technological singularity. <br><br><strong>The "Meta-intelligence" argument</strong><br><br>One of the strongest arguments against the idea of a technological singularity in my view is that it confuses intelligence to do a task with the capability to improve your intelligence to do a task. David Chalmers, in an otherwise careful analysis of the idea of a technological singularity, writes: <br><br> <blockquote>"If we produce an AI by machine learning, it is likely that soon after we will be able to improve the learning algorithm and extend the learning process, leading to AI+" (Chalmers 2010)</blockquote><br><br>Here, AI is a system with human level intelligence and AI+ is a system more intelligent than the most intelligent human. But why should it be likely that soon after we can improve the learning algorithm? Progress in machine learning algorithms has neither been especially rapid or easy. Machine learning is indeed likely to be a significant component of any human level AI system that we might build in the future if only because it will be painful to hand code its knowledge and expertise otherwise. Suppose an AI system uses machine learning to improve its performance at some tasks requiring intelligence like understanding a text, or proving mathematical identities. There is no reason that the system can in addition improve the fundamental machine learning algorithm used to do this. Machine learning algorithms frequently top out a particular task, and no amount of tweaking, be it feature engineering or parameter tuning, appears able to improve their performance. <br><br>We are currently seeing impressive advances in AI using deep learning (Edwards 2015). This has dramatically improved the state-of-the-art in speech recognition, computer vision, natural language processing and a number of other domains. These improvements have come largely from using larger data sets, and deeper neural networks: <br><br> <blockquote>""Before, neural networks were not breaking records for recognizing continuous speech; they were not big enough." Yann LeCun, quoted in (Edwards 2015)</blockquote><br><br>Of course, more data and bigger neural networks means we need more processing power. As a result, GPUs are now frequently used to provide this processing power. However, being better able to recognize speech or identify objects has not lead to an improvement in deep learning itself. The deep learning algorithms have not improved themselves. Any improvements to the deep learning algorithms have been hard won by applying our own intelligence to their design. <br><br>We can come at this argument from another direction using one of the best examples we know of intelligent systems. Look at ourselves. We only use a fraction of the capabilities of our amazing brains, and we struggle to change this. It is much easier for us to learn how to do better at a particular task, than it is for us to learn how to learn better in general. For instance, if we remove the normalization inherent in the definition of IQ, we can observe that IQ has increased over the last century but only slowly (the "Flynn effect"). And improving your IQ today is pretty much as slow and painful as it was a century ago. Perhaps electronic brains will also struggle to improve their performance quickly and never get beyond a fraction of their fundamental capabilities? <br><br><strong>The "Diminishing returns" argument</strong><br><br>The idea of a technological singularity typically supposes improvements to intelligence will be a relative constant multiplier, each generation getting some fraction better that the last. However, the performance so far of most of our AI systems has been that of diminishing returns. There is often lots of low hanging fruit at the start, but we then run into great difficulties to improve after this. This helps explain the overly optimistic claims made by many of the early AI researchers. An AI system may be able to improve itself an infinite number of times, but the extent to which its intelligence changes overall could be bounded. For instance, if each generation only improves by half the last change, then the system will never get beyond doubling its overall intelligence. <br><br>Diminishing returns may also come not from the difficulty of improving our AI algorithms, but from the difficulty of their subject matter increasing rapidly. Paul Allen, the Microsoft co-founder calls this the "complexity brake". <br><br><blockquote> "We call this issue the complexity brake. As we go deeper and deeper in our understanding of natural systems, we typically find that we require more and more specialized knowledge to characterize them, and we are forced to continuously expand our scientific theories in more and more complex ways . . . we believe that progress toward this understanding [of cognition] is fundamentally slowed by the complexity brake." (Allen and Greaves 2011)</blockquote><br><br>Even if we see continual, perhaps even exponential improvements in our AI systems, this may not be enough to improve performance. The difficulty of the problems required to be solved to see intelligence increase may themselves increase even more rapidly. There are those that argue theoretical physics appears to be running into such complexity brakes. <br><br><strong>The "Limits of intelligence" argument</strong><br><br>There are many fundamental limits within the universe. Some of these are physical. You cannot accelerate past the speed of light. You cannot know both position and momentum with complete accuracy. You cannot know when the radioactive decay of an atom will happen with certainty. Any thinking machine that we might build will be limited by these physical laws. Of course, if that machine is electronic or even quantum in nature, these limits are likely to be much greater than the biological and chemical limits of our human brains. <br><br>There are also more empirical laws which can be observed emerging out of complex systems. For example, Dunbar\'s number is the observed correlation between brain size for primates and average social group size. This puts a limit of between 100 and 250 stable relationships on human social groups. Intelligence is also a complex phenomenon and may also have such limits which emerge from this complexity. Any improvements in machine intelligence, whether it runs away or happens more slowly, may run into such limits. Or course, there is no reason to suppose that our own human intelligence is at or close to this limit. But equally, there\'s little reason why any such limits are necessarily far beyond our own intelligence. <br><br><strong>The "Computational complexity" argument</strong><br><br>Suppose we stick to building AI systems with computers that obey our traditional models of computation. Even exponential improvements are no match for computational complexity. For instance, exponential growth in performance is inadequate to run super-exponential algorithms. And no amount of growth in performance will make undecidable problems decidable. Computational complexity may be one of the fundamental limits discussed in the previous argument. Hence, unless we use machines that beyond our traditional models of computation, we are likely to bump into many problems where computational complexity fundamentally limits performance. Of course, a lot of computational complexity is about worst case, and much of AI is about using heuristics to solve problems in practice that are computationally intractable in the worst case. There are, however, fundamental limits to the quality of these heuristics. There will be classes of problems that even a super-human intelligence cannot solve well, even approximately. <br><br><strong>Conclusions</strong><br><br>I have argued that there are many reasons why we might never witness a technological signularity. Nevertheless, even without a technological singularity, we might still end up with machines that exhibit super-human levels of intelligence. We might just have to program much of this painfully ourselves. If this is the case, the impact of AI on our economy, and on our society may be less dramatic than either the pessimists or the optimists have predicted. Nevertheless, we should start planning for the impact that AI will have on society. Even without a technological singularity, AI is likely to have a large impact on the nature of work. As a second example, even quite limited AI is likely to have a large impact on the nature of war. We need to start planning today for this future. <br><br><strong>References</strong><br><br> [Allen and Greaves 2011] Allen, P., and Greaves, M. 2011. The singularity isn\'t near. MIT Technology Review 7–65. <br><br> [Bostrom 2002] Bostrom, N. 2002. When machines outsmart humans. Futures 35(7):759–764. <br><br> [Bostrom 2014] Bostrom, N. 2014. Superintelligence: Paths, Dangers, Strategies. Oxford, UK: Oxford University Press. <br><br> [Chalmers 2010] Chalmers, D. 2010. The singularity: A philosophical analysis. Journal of Consciousness Studies 17(9-10):7–65. <br><br> [Edwards 2015] Edwards, C. 2015. Growing pains for deep learning. Commun. ACM 58(7):14–16. <br><br> [Good 1965] Good, I. 1965. Speculations concerning the first ultraintelligent machine. Advances in Computers 6:31– 88. <br><br> [Kurzweil 2006] Kurzweil, R. 2006. The Singularity Is Near: When Humans Transcend Biology. Penguin (NonClassics). <br><br> [Pinker 2008] Pinker, S. 2008. Tech luminaries address singularity. IEEE Spectrum. <br><br> [Turing 1963] Turing, A. 1963. Computing machinery and intelligence. In Computers and Thought. McGraw-Hill Book Company. <br><br> [Ulam 1958] Ulam, S. 1958. Tribute to John von Neumann. Bulletin of the American Mathematical Society 64(3). <br><br> [Vinge 1993] Vinge, V. 1993. The coming technological singularity: How to survive in the post-human era. In Rheingold, H., ed., Whole Earth Review.</p><button class = "btn btn-info" onClick="hideSingularityMayNever()">Hide Source</button>';
  document.getElementById("divSingularityMayNever").className = "col-lg-12 col-md-12 col-sm-12 col-xs-12";
}
function hideSingularityMayNever() {
  document.getElementById("singularityMayNever").innerHTML='<button class = "btn btn-primary" onClick="showSingularityMayNever()">Show Source</button>';
  document.getElementById("divSingularityMayNever").className = "col-lg-6 col-md-6 col-sm-12 col-xs-12";
}


function showManImmortal() {
  document.getElementById("manImmortal").innerHTML='<button class = "btn btn-info" onClick="hideManImmortal()">Hide Source</button><p>On Feb. 15, 1965, a diffident but self-possessed high school student named Raymond Kurzweil appeared as a guest on a game show called I\'ve Got a Secret. He was introduced by the host, Steve Allen, then he played a short musical composition on a piano. The idea was that Kurzweil was hiding an unusual fact and the panelists - they included a comedian and a former Miss America - had to guess what it was.<br><br> On the show (see the clip on YouTube), the beauty queen did a good job of grilling Kurzweil, but the comedian got the win: the music was composed by a computer. Kurzweil got $200. <br><br> Kurzweil then demonstrated the computer, which he built himself - a desk-size affair with loudly clacking relays, hooked up to a typewriter. The panelists were pretty blasé about it; they were more impressed by Kurzweil\'s age than by anything he\'d actually done. They were ready to move on to Mrs. Chester Loney of Rough and Ready, Calif., whose secret was that she\'d been President Lyndon Johnson\'s first-grade teacher. <br><br> But Kurzweil would spend much of the rest of his career working out what his demonstration meant. Creating a work of art is one of those activities we reserve for humans and humans only. It\'s an act of self-expression; you\'re not supposed to be able to do it if you don\'t have a self. To see creativity, the exclusive domain of humans, usurped by a computer built by a 17-year-old is to watch a line blur that cannot be unblurred, the line between organic intelligence and artificial intelligence. <br><br> That was Kurzweil\'s real secret, and back in 1965 nobody guessed it. Maybe not even him, not yet. But now, 46 years later, Kurzweil believes that we\'re approaching a moment when computers will become intelligent, and not just intelligent but more intelligent than humans. When that happens, humanity - our bodies, our minds, our civilization - will be completely and irreversibly transformed. He believes that this moment is not only inevitable but imminent. According to his calculations, the end of human civilization as we know it is about 35 years away. <br><br> Computers are getting faster. Everybody knows that. Also, computers are getting faster faster - that is, the rate at which they\'re getting faster is increasing. <br><br> True? True. <br><br> So if computers are getting so much faster, so incredibly fast, there might conceivably come a moment when they are capable of something comparable to human intelligence. Artificial intelligence. All that horsepower could be put in the service of emulating whatever it is our brains are doing when they create consciousness - not just doing arithmetic very quickly or composing piano music but also driving cars, writing books, making ethical decisions, appreciating fancy paintings, making witty observations at cocktail parties. <br><br> If you can swallow that idea, and Kurzweil and a lot of other very smart people can, then all bets are off. From that point on, there\'s no reason to think computers would stop getting more powerful. They would keep on developing until they were far more intelligent than we are. Their rate of development would also continue to increase, because they would take over their own development from their slower-thinking human creators. Imagine a computer scientist that was itself a super-intelligent computer. It would work incredibly quickly. It could draw on huge amounts of data effortlessly. It wouldn\'t even take breaks to play Farmville. <br><br> Probably. It\'s impossible to predict the behavior of these smarter-than-human intelligences with which (with whom?) we might one day share the planet, because if you could, you\'d be as smart as they would be. But there are a lot of theories about it. Maybe we\'ll merge with them to become super-intelligent cyborgs, using computers to extend our intellectual abilities the same way that cars and planes extend our physical abilities. Maybe the artificial intelligences will help us treat the effects of old age and prolong our life spans indefinitely. Maybe we\'ll scan our consciousnesses into computers and live inside them as software, forever, virtually. Maybe the computers will turn on humanity and annihilate us. The one thing all these theories have in common is the transformation of our species into something that is no longer recognizable as such to humanity circa 2011. This transformation has a name: the Singularity. <br><br> The difficult thing to keep sight of when you\'re talking about the Singularity is that even though it sounds like science fiction, it isn\'t, no more than a weather forecast is science fiction. It\'s not a fringe idea; it\'s a serious hypothesis about the future of life on Earth. There\'s an intellectual gag reflex that kicks in anytime you try to swallow an idea that involves super-intelligent immortal cyborgs, but suppress it if you can, because while the Singularity appears to be, on the face of it, preposterous, it\'s an idea that rewards sober, careful evaluation. <br><br> People are spending a lot of money trying to understand it. The three-year-old Singularity University, which offers inter-disciplinary courses of study for graduate students and executives, is hosted by NASA. Google was a founding sponsor; its CEO and co-founder Larry Page spoke there last year. People are attracted to the Singularity for the shock value, like an intellectual freak show, but they stay because there\'s more to it than they expected. And of course, in the event that it turns out to be real, it will be the most important thing to happen to human beings since the invention of language. <br><br> The Singularity isn\'t a wholly new idea, just newish. In 1965 the British mathematician I.J. Good described something he called an "intelligence explosion":<br><br><blockquote> Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an "intelligence explosion," and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make.</blockquote><br><br> The word singularity is borrowed from astrophysics: it refers to a point in space-time - for example, inside a black hole - at which the rules of ordinary physics do not apply. In the 1980s the science-fiction novelist Vernor Vinge attached it to Good\'s intelligence-explosion scenario. At a NASA symposium in 1993, Vinge announced that "within 30 years, we will have the technological means to create super-human intelligence. Shortly after, the human era will be ended." <br><br> By that time Kurzweil was thinking about the Singularity too. He\'d been busy since his appearance on I\'ve Got a Secret. He\'d made several fortunes as an engineer and inventor; he founded and then sold his first software company while he was still at MIT. He went on to build the first print-to-speech reading machine for the blind - Stevie Wonder was customer No. 1 - and made innovations in a range of technical fields, including music synthesizers and speech recognition. He holds 39 patents and 19 honorary doctorates. In 1999 President Bill Clinton awarded him the National Medal of Technology. <br><br> But Kurzweil was also pursuing a parallel career as a futurist: he has been publishing his thoughts about the future of human and machine-kind for 20 years, most recently in The Singularity Is Near, which was a best seller when it came out in 2005. A documentary by the same name, starring Kurzweil, Tony Robbins and Alan Dershowitz, among others, was released in January. (Kurzweil is actually the subject of two current documentaries. The other one, less authorized but more informative, is called The Transcendent Man.) Bill Gates has called him "the best person I know at predicting the future of artificial intelligence." <br><br>In real life, the transcendent man is an unimposing figure who could pass for Woody Allen\'s even nerdier younger brother. Kurzweil grew up in Queens, N.Y., and you can still hear a trace of it in his voice. Now 62, he speaks with the soft, almost hypnotic calm of someone who gives 60 public lectures a year. As the Singularity\'s most visible champion, he has heard all the questions and faced down the incredulity many, many times before. He\'s good-natured about it. His manner is almost apologetic: I wish I could bring you less exciting news of the future, but I\'ve looked at the numbers, and this is what they say, so what else can I tell you? <br><br> Kurzweil\'s interest in humanity\'s cyborganic destiny began about 1980 largely as a practical matter. He needed ways to measure and track the pace of technological progress. Even great inventions can fail if they arrive before their time, and he wanted to make sure that when he released his, the timing was right. "Even at that time, technology was moving quickly enough that the world was going to be different by the time you finished a project," he says. "So it\'s like skeet shooting - you can\'t shoot at the target." He knew about Moore\'s law, of course, which states that the number of transistors you can put on a microchip doubles about every two years. It\'s a surprisingly reliable rule of thumb. Kurzweil tried plotting a slightly different curve: the change over time in the amount of computing power, measured in MIPS (millions of instructions per second), that you can buy for $1,000. <br><br> As it turned out, Kurzweil\'s numbers looked a lot like Moore\'s. They doubled every couple of years. Drawn as graphs, they both made exponential curves, with their value increasing by multiples of two instead of by regular increments in a straight line. The curves held eerily steady, even when Kurzweil extended his backward through the decades of pretransistor computing technologies like relays and vacuum tubes, all the way back to 1900. <br><br> Kurzweil then ran the numbers on a whole bunch of other key technological indexes - the falling cost of manufacturing transistors, the rising clock speed of microprocessors, the plummeting price of dynamic RAM. He looked even further afield at trends in biotech and beyond - the falling cost of sequencing DNA and of wireless data service and the rising numbers of Internet hosts and nanotechnology patents. He kept finding the same thing: exponentially accelerating progress. "It\'s really amazing how smooth these trajectories are," he says. "Through thick and thin, war and peace, boom times and recessions." Kurzweil calls it the law of accelerating returns: technological progress happens exponentially, not linearly. <br><br> Then he extended the curves into the future, and the growth they predicted was so phenomenal, it created cognitive resistance in his mind. Exponential curves start slowly, then rocket skyward toward infinity. According to Kurzweil, we\'re not evolved to think in terms of exponential growth. "It\'s not intuitive. Our built-in predictors are linear. When we\'re trying to avoid an animal, we pick the linear prediction of where it\'s going to be in 20 seconds and what to do about it. That is actually hardwired in our brains." <br><br> Here\'s what the exponential curves told him. We will successfully reverse-engineer the human brain by the mid-2020s. By the end of that decade, computers will be capable of human-level intelligence. Kurzweil puts the date of the Singularity - never say he\'s not conservative - at 2045. In that year, he estimates, given the vast increases in computing power and the vast reductions in the cost of same, the quantity of artificial intelligence created will be about a billion times the sum of all the human intelligence that exists today. <br><br> The Singularity isn\'t just an idea. it attracts people, and those people feel a bond with one another. Together they form a movement, a subculture; Kurzweil calls it a community. Once you decide to take the Singularity seriously, you will find that you have become part of a small but intense and globally distributed hive of like-minded thinkers known as Singularitarians. <br><br> Not all of them are Kurzweilians, not by a long chalk. There\'s room inside Singularitarianism for considerable diversity of opinion about what the Singularity means and when and how it will or won\'t happen. But Singularitarians share a worldview. They think in terms of deep time, they believe in the power of technology to shape history, they have little interest in the conventional wisdom about anything, and they cannot believe you\'re walking around living your life and watching TV as if the artificial-intelligence revolution were not about to erupt and change absolutely everything. They have no fear of sounding ridiculous; your ordinary citizen\'s distaste for apparently absurd ideas is just an example of irrational bias, and Singularitarians have no truck with irrationality. When you enter their mind-space you pass through an extreme gradient in worldview, a hard ontological shear that separates Singularitarians from the common run of humanity. Expect turbulence. <br><br> In addition to the Singularity University, which Kurzweil co-founded, there\'s also a Singularity Institute for Artificial Intelligence, based in San Francisco. It counts among its advisers Peter Thiel, a former CEO of PayPal and an early investor in Facebook. The institute holds an annual conference called the Singularity Summit. (Kurzweil co-founded that too.) Because of the highly interdisciplinary nature of Singularity theory, it attracts a diverse crowd. Artificial intelligence is the main event, but the sessions also cover the galloping progress of, among other fields, genetics and nanotechnology. <br><br> At the 2010 summit, which took place in August in San Francisco, there were not just computer scientists but also psychologists, neuroscientists, nanotechnologists, molecular biologists, a specialist in wearable computers, a professor of emergency medicine, an expert on cognition in gray parrots and the professional magician and debunker James "the Amazing" Randi. The atmosphere was a curious blend of Davos and UFO convention. Proponents of seasteading - the practice, so far mostly theoretical, of establishing politically autonomous floating communities in international waters - handed out pamphlets. An android chatted with visitors in one corner. <br><br> After artificial intelligence, the most talked-about topic at the 2010 summit was life extension. Biological boundaries that most people think of as permanent and inevitable Singularitarians see as merely intractable but solvable problems. Death is one of them. Old age is an illness like any other, and what do you do with illnesses? You cure them. Like a lot of Singularitarian ideas, it sounds funny at first, but the closer you get to it, the less funny it seems. It\'s not just wishful thinking; there\'s actual science going on here. <br><br> For example, it\'s well known that one cause of the physical degeneration associated with aging involves telomeres, which are segments of DNA found at the ends of chromosomes. Every time a cell divides, its telomeres get shorter, and once a cell runs out of telomeres, it can\'t reproduce anymore and dies. But there\'s an enzyme called telomerase that reverses this process; it\'s one of the reasons cancer cells live so long. So why not treat regular non-cancerous cells with telomerase? In November, researchers at Harvard Medical School announced in Nature that they had done just that. They administered telomerase to a group of mice suffering from age-related degeneration. The damage went away. The mice didn\'t just get better; they got younger. <br><br> Aubrey de Grey is one of the world\'s best-known life-extension researchers and a Singularity Summit veteran. A British biologist with a doctorate from Cambridge and a famously formidable beard, de Grey runs a foundation called SENS, or Strategies for Engineered Negligible Senescence. He views aging as a process of accumulating damage, which he has divided into seven categories, each of which he hopes to one day address using regenerative medicine. "People have begun to realize that the view of aging being something immutable - rather like the heat death of the universe - is simply ridiculous," he says. "It\'s just childish. The human body is a machine that has a bunch of functions, and it accumulates various types of damage as a side effect of the normal function of the machine. Therefore in principal that damage can be repaired periodically. This is why we have vintage cars. It\'s really just a matter of paying attention. The whole of medicine consists of messing about with what looks pretty inevitable until you figure out how to make it not inevitable." <br><br> Kurzweil takes life extension seriously too. His father, with whom he was very close, died of heart disease at 58. Kurzweil inherited his father\'s genetic predisposition; he also developed Type 2 diabetes when he was 35. Working with Terry Grossman, a doctor who specializes in longevity medicine, Kurzweil has published two books on his own approach to life extension, which involves taking up to 200 pills and supplements a day. He says his diabetes is essentially cured, and although he\'s 62 years old from a chronological perspective, he estimates that his biological age is about 20 years younger. <br><br> But his goal differs slightly from de Grey\'s. For Kurzweil, it\'s not so much about staying healthy as long as possible; it\'s about staying alive until the Singularity. It\'s an attempted handoff. Once hyper-intelligent artificial intelligences arise, armed with advanced nanotechnology, they\'ll really be able to wrestle with the vastly complex, systemic problems associated with aging in humans. Alternatively, by then we\'ll be able to transfer our minds to sturdier vessels such as computers and robots. He and many other Singularitarians take seriously the proposition that many people who are alive today will wind up being functionally immortal. <br><br> It\'s an idea that\'s radical and ancient at the same time. In "Sailing to Byzantium," W.B. Yeats describes mankind\'s fleshly predicament as a soul fastened to a dying animal. Why not unfasten it and fasten it to an immortal robot instead? But Kurzweil finds that life extension produces even more resistance in his audiences than his exponential growth curves. "There are people who can accept computers being more intelligent than people," he says. "But the idea of significant changes to human longevity - that seems to be particularly controversial. People invested a lot of personal effort into certain philosophies dealing with the issue of life and death. I mean, that\'s the major reason we have religion." <br><br> Of course, a lot of people think the Singularity is nonsense - a fantasy, wishful thinking, a Silicon Valley version of the Evangelical story of the Rapture, spun by a man who earns his living making outrageous claims and backing them up with pseudoscience. Most of the serious critics focus on the question of whether a computer can truly become intelligent. <br><br> The entire field of artificial intelligence, or AI, is devoted to this question. But AI doesn\'t currently produce the kind of intelligence we associate with humans or even with talking computers in movies - HAL or C3PO or Data. Actual AIs tend to be able to master only one highly specific domain, like interpreting search queries or playing chess. They operate within an extremely specific frame of reference. They don\'t make conversation at parties. They\'re intelligent, but only if you define intelligence in a vanishingly narrow way. The kind of intelligence Kurzweil is talking about, which is called strong AI or artificial general intelligence, doesn\'t exist yet. <br><br> Why not? Obviously we\'re still waiting on all that exponentially growing computing power to get here. But it\'s also possible that there are things going on in our brains that can\'t be duplicated electronically no matter how many MIPS you throw at them. The neurochemical architecture that generates the ephemeral chaos we know as human consciousness may just be too complex and analog to replicate in digital silicon. The biologist Dennis Bray was one of the few voices of dissent at last summer\'s Singularity Summit. "Although biological components act in ways that are comparable to those in electronic circuits," he argued, in a talk titled "What Cells Can Do That Robots Can\'t," "they are set apart by the huge number of different states they can adopt. Multiple biochemical processes create chemical modifications of protein molecules, further diversified by association with distinct structures at defined locations of a cell. The resulting combinatorial explosion of states endows living systems with an almost infinite capacity to store information regarding past and present conditions and a unique capacity to prepare for future events." That makes the ones and zeros that computers trade in look pretty crude. <br><br> Underlying the practical challenges are a host of philosophical ones. Suppose we did create a computer that talked and acted in a way that was indistinguishable from a human being - in other words, a computer that could pass the Turing test. (Very loosely speaking, such a computer would be able to pass as human in a blind test.) Would that mean that the computer was sentient, the way a human being is? Or would it just be an extremely sophisticated but essentially mechanical automaton without the mysterious spark of consciousness - a machine with no ghost in it? And how would we know? <br><br> Even if you grant that the Singularity is plausible, you\'re still staring at a thicket of unanswerable questions. If I can scan my consciousness into a computer, am I still me? What are the geopolitics and the socioeconomics of the Singularity? Who decides who gets to be immortal? Who draws the line between sentient and nonsentient? And as we approach immortality, omniscience and omnipotence, will our lives still have meaning? By beating death, will we have lost our essential humanity? <br><br> Kurzweil admits that there\'s a fundamental level of risk associated with the Singularity that\'s impossible to refine away, simply because we don\'t know what a highly advanced artificial intelligence, finding itself a newly created inhabitant of the planet Earth, would choose to do. It might not feel like competing with us for resources. One of the goals of the Singularity Institute is to make sure not just that artificial intelligence develops but also that the AI is friendly. You don\'t have to be a super-intelligent cyborg to understand that introducing a superior life-form into your own biosphere is a basic Darwinian error. <br><br> If the Singularity is coming, these questions are going to get answers whether we like it or not, and Kurzweil thinks that trying to put off the Singularity by banning technologies is not only impossible but also unethical and probably dangerous. "It would require a totalitarian system to implement such a ban," he says. "It wouldn\'t work. It would just drive these technologies underground, where the responsible scientists who we\'re counting on to create the defenses would not have easy access to the tools." <br><br> Kurzweil is an almost inhumanly patient and thorough debater. He relishes it. He\'s tireless in hunting down his critics so that he can respond to them, point by point, carefully and in detail. <br><br> Take the question of whether computers can replicate the biochemical complexity of an organic brain. Kurzweil yields no ground there whatsoever. He does not see any fundamental difference between flesh and silicon that would prevent the latter from thinking. He defies biologists to come up with a neurological mechanism that could not be modeled or at least matched in power and flexibility by software running on a computer. He refuses to fall on his knees before the mystery of the human brain. "Generally speaking," he says, "the core of a disagreement I\'ll have with a critic is, they\'ll say, Oh, Kurzweil is underestimating the complexity of reverse-engineering of the human brain or the complexity of biology. But I don\'t believe I\'m underestimating the challenge. I think they\'re underestimating the power of exponential growth." <br><br> This position doesn\'t make Kurzweil an outlier, at least among Singularitarians. Plenty of people make more-extreme predictions. Since 2005 the neuroscientist Henry Markram has been running an ambitious initiative at the Brain Mind Institute of the Ecole Polytechnique in Lausanne, Switzerland. It\'s called the Blue Brain project, and it\'s an attempt to create a neuron-by-neuron simulation of a mammalian brain, using IBM\'s Blue Gene super-computer. So far, Markram\'s team has managed to simulate one neocortical column from a rat\'s brain, which contains about 10,000 neurons. Markram has said that he hopes to have a complete virtual human brain up and running in 10 years. (Even Kurzweil sniffs at this. If it worked, he points out, you\'d then have to educate the brain, and who knows how long that would take?) <br><br> By definition, the future beyond the Singularity is not knowable by our linear, chemical, animal brains, but Kurzweil is teeming with theories about it. He positively flogs himself to think bigger and bigger; you can see him kicking against the confines of his aging organic hardware. "When people look at the implications of ongoing exponential growth, it gets harder and harder to accept," he says. "So you get people who really accept, yes, things are progressing exponentially, but they fall off the horse at some point because the implications are too fantastic. I\'ve tried to push myself to really look." <br><br> In Kurzweil\'s future, biotechnology and nanotechnology give us the power to manipulate our bodies and the world around us at will, at the molecular level. Progress hyperaccelerates, and every hour brings a century\'s worth of scientific breakthroughs. We ditch Darwin and take charge of our own evolution. The human genome becomes just so much code to be bug-tested and optimized and, if necessary, rewritten. Indefinite life extension becomes a reality; people die only if they choose to. Death loses its sting once and for all. Kurzweil hopes to bring his dead father back to life. <br><br> We can scan our consciousnesses into computers and enter a virtual existence or swap our bodies for immortal robots and light out for the edges of space as intergalactic godlings. Within a matter of centuries, human intelligence will have re-engineered and saturated all the matter in the universe. This is, Kurzweil believes, our destiny as a species. <br><br> Or it isn\'t. When the big questions get answered, a lot of the action will happen where no one can see it, deep inside the black silicon brains of the computers, which will either bloom bit by bit into conscious minds or just continue in ever more brilliant and powerful iterations of nonsentience. <br><br> But as for the minor questions, they\'re already being decided all around us and in plain sight. The more you read about the Singularity, the more you start to see it peeking out at you, coyly, from unexpected directions. Five years ago we didn\'t have 600 million humans carrying out their social lives over a single electronic network. Now we have Facebook. Five years ago you didn\'t see people double-checking what they were saying and where they were going, even as they were saying it and going there, using handheld network-enabled digital prosthetics. Now we have iPhones. Is it an unimaginable step to take the iPhones out of our hands and put them into our skulls? <br><br> Already 30,000 patients with Parkinson\'s disease have neural implants. Google is experimenting with computers that can drive cars. There are more than 2,000 robots fighting in Afghanistan alongside the human troops. This month a game show will once again figure in the history of artificial intelligence, but this time the computer will be the guest: an IBM super-computer nicknamed Watson will compete on Jeopardy! Watson runs on 90 servers and takes up an entire room, and in a practice match in January it finished ahead of two former champions, Ken Jennings and Brad Rutter. It got every question it answered right, but much more important, it didn\'t need help understanding the questions (or, strictly speaking, the answers), which were phrased in plain English. Watson isn\'t strong AI, but if strong AI happens, it will arrive gradually, bit by bit, and this will have been one of the bits. <br><br> A hundred years from now, Kurzweil and de Grey and the others could be the 22nd century\'s answer to the Founding Fathers - except unlike the Founding Fathers, they\'ll still be alive to get credit - or their ideas could look as hilariously retro and dated as Disney\'s Tomorrowland. Nothing gets old as fast as the future. <br><br> But even if they\'re dead wrong about the future, they\'re right about the present. They\'re taking the long view and looking at the big picture. You may reject every specific article of the Singularitarian charter, but you should admire Kurzweil for taking the future seriously. Singularitarianism is grounded in the idea that change is real and that humanity is in charge of its own fate and that history might not be as simple as one damn thing after another. Kurzweil likes to point out that your average cell phone is about a millionth the size of, a millionth the price of and a thousand times more powerful than the computer he had at MIT 40 years ago. Flip that forward 40 years and what does the world look like? If you really want to figure that out, you have to think very, very far outside the box. Or maybe you have to think further inside it than anyone ever has before. <br><br> </p><button class = "btn btn-info" onClick="hideManImmortal()">Hide Source</button>';
  document.getElementById("divManImmortal").className = "col-lg-12 col-md-12 col-sm-12 col-xs-12";
}
function hideManImmortal() {
  document.getElementById("manImmortal").innerHTML='<button class = "btn btn-primary" onClick="showManImmortal()">Show Source</button>';
  document.getElementById("divManImmortal").className = "col-lg-6 col-md-6 col-sm-12 col-xs-12";
}


function showDebunkingAI() {
  document.getElementById("debunkingAI").innerHTML='<button class = "btn btn-info" onClick="hideDebunkingAI()">Hide Source</button><p>The concept of inhuman intelligence goes back to the deep prehistory of mankind. At first the province of gods, demons, and spirits, it transferred seamlessly into the interlinked worlds of magic and technology. Ancient Greek myths had numerous robots, made variously by gods or human inventors, while extant artefacts like the Antikythera calendrical computer show that even in 200 BCE we could build machinery that usefully mimicked human intellectual abilities.<br><br>There has been no age or civilisation without a popular concept of artificial intelligence (AI). Ours, however, is the first where the genuine article machinery that comfortably exceeds our own thinking skills is not only possible but achievable. It should not be a surprise, then, that our ideas of what that actuallymeans and what will actually happen are hopelessly coloured by cultural assumptions ancient and modern.<br><br>We rarely get it right: Kubrick\'s 2001 saw HAL 9000 out-thinking highly trained astronauts to murderous effect; Bill Gates\' 2001 gave us Clippy, which was more easily dealt with.<br><br>Now, with AI a multi-billion dollar industry seeping into our phones, businesses, cars, and homes, it\'s time to bust some of the most important AI myths and dip into some reality.<br><br><strong>Myth: AI is all about making machines that can think</strong><br><br>When digital computing first became practical in the middle of the last century, there were high hopes that AI would follow in short order. Alan Turing was famously comfortable with the concept in his 1948 "Intelligent Machinery" paper, seeing no objections to a working, thinking machine by the end of the century. Sci-fi author Isaac Asimov created Multivac, a larger, brighter version of actual computers such as the 1951 UNIVAC 1, first used by the US Census Bureau. (The favour was later returned by IBM, when it named the first chess computer to outrank all humans Deep Blue after Douglas Adams\' hyperintelligent Deep Thought machine from the HitchHiker\'s Guide To The Galaxy.)<br><br>There are many projects and much research going on into replicating human-like thought, mostly by hardware and software simulations of human brain structures and functions as new techniques reveal them. One of the higher-profile efforts is the Blue Brain project at the Brain and Mind Institute of the École Polytechnique Fédérale de Lausanne (EPFL) in Switzerland, which started in 2005 with a target date for a working model roughly equivalent to some human functions by 2023.<br><br>There are two main problems for any brain simulator. The first is that the human brain is extraordinarily complex, with around 100 billion neurons and 1,000 trillion synaptic interconnections. None of this is digital; it depends on electrochemical signaling with inter-related timing and analogue components, the sort of molecular and biological machinery that we are only just starting to understand.<br><br>Even much simpler brains remain mysterious. The landmark success to date for Blue Brain, reported this year, has been a small 30,000 neuron section of a rat brain that replicates signals seen in living rodents. 30,000 is just a tiny fraction of a complete mammalian brain, and as the number of neurons and interconnecting synapses increases, so the simulation becomes exponentially more complex and exponentially beyond our current technological reach.<br><br>This yawning chasm of understanding leads to the second big problem: there is no accepted theory of mind that describes what "thought" actually is.<br><br>This underlying quandary attempting to define "thought" is sometimes referred to as the hard problem, and the results of understanding it are called strong AI. People engaged in commercial AI remain sceptical that it will be resolved any time soon, or that it is necessary or even desirable to do for any practical benefits. There is no doubt that artificial intelligences are beginning to do very meaningful work and that the speed of change of technology will continue to shunt things along, but full-blown sentience still seems far-fetched.<br><br>IBM Watson, one of the highest-profile successes in AI to date, started its life as an artificial contender on the American TV game show Jeopardy. It combines natural language processing with a large number of expert processes that try different strategies to match an internal knowledge database with potential answers. It then checks the confidence levels of its internal experts and chooses to answer the question if those levels are high enough (see below right).<br><br>The first serious application of Watson that might actually improve the quality of human life has been as a diagnostic aid in cancer medicine. Since 2011, Watson has been assisting oncologists by delving through patient medical records and trying to correlate that data with clinical expertise, academic research, or other sources of data in its memory banks. The end result is that Watson might offer up treatment options that the human doctor may not have previously considered.<br><br>"[It\'s like] having a capable and knowledgeable \'colleague\' who can review the current information that relates to my patient," said Dr. James Miser, the chief medical information officer at Bumrungrad international hospital in Thailand. "It is fast, thorough, and has the uncanny ability to understand how the available evidence applies to the unique individual I am treating."<br><br>As marvellous as this sounds, it mostly serves to highlight the similarities and differences between current, narrow, practical AI and its strong, as-yet-mythical cousin. One basic engine of both is the neural network, a system based on basic biological concepts that takes a set of inputs and attempts to match them to things that have previously been seen by the neural network. The key concept is that the system isn\'t told how to do this analysis; instead, it learns by being given both inputs and outputs for a correct solution and then adjusting its own computational pathways to create internal knowledge to be used on later, unknown inputs.<br><br>We are now at the point where Watson and other AI systems such as Facebook\'s DeepFace facial recognition system can do this with narrow, constrained data sets, but they are generally incapable by themselves of extending beyond the very specific tasks they\'ve been programmed to do.<br><br>Google, for its part, seems content with narrow AI searching pictures by content, crunching environmental and science data, and machine language translation than predicting the emergence of general strong AI. The human brain can find, utilise, and link together vastly more complicated and ill-defined data, performing feats of recognition and transformation that can model entire universes. Google projects like DeepMind are experimenting with combining different techniques in one case, using neural networks alongside reinforcement learning where the machine generates random inputs until it happens to hit on a rewarding strategy, which it refines to try and close the gap, but they still act on very specific, narrow tasks.<br><br>Most recently, the DeepMind project used this combination of techniques to "master a diverse range of Atari 2600 games." Speaking to Wired, Google researcher Koray Kavukcuoglu said his team has built "a general-learning algorithm that should be applicable to many other tasks" but learning how to perform a task is a long way away from consciously thinking about those tasks, what the repercussions of those tasks might be, or having the wherewithal to opt out of doing those tasks in the first place.<strong>Myth: AI won\'t be bound by human ethics</strong>The myriad dangers of artificial intelligences acting independently from humans are easy to imagine in the case of a rogue robot warrior, or a self-driving car that doesn\'t correctly identify a life-threatening situation. The dangers are less obvious in the case of a smart search engine that has been quietly biased to give answers that, in the humble opinion of the megacorp that owns the search engine, aren\'t in your best interest.<br><br>These are real worries with immediate importance to how we use, and are used by, the current and plausible future of AI technology. If a doctor uses Watson (or Siri or Google Now or Cortana) as part of what proves to be a misdiagnosis, who or what is ethically responsible for the consequences? And might we one day face the issues of sentient machines demanding rights?<br><br>The good news is that these worries are being taken seriously. Trying to define ethics, even between humans, is notoriously difficult. Society\'s generally accepted ground rules are codified in a practical way by law and the legal system and it\'s here that practical answers to AI ethics are being developed.<br><br>The first question is whether robots and AI are genuinely new things in human experience requiring new ways of thinking, or whether they can be corralled by tweaks to existing principles.<br><br>"Both," Ryan Calo, assistant professor of law at Washington University and leading light of cyberlaw, told Ars Technica. "Some rather visible people focus on the notion that robots will \'wake up\' and demand rights or try to harm us. I don\'t think this will happen, at least not in the foreseeable future. But robots and AI even now present novel and interesting challenges for law and policy, just as the Internet did in the 1990s."<br><br>So what happens if an AI learns or exhibits harmful behaviour. Who carries the can?<br><br>We have options, said Calo, including making people strictly liable if they deploy learning systems where they could cause trouble. "This could limit self-learning systems to those where they are really needed or less dangerous," he said. But that can\'t cover everything, according to Calo. "Risk management will play an even greater role in technology policy."<br><br>The Internet itself, a new technology that brought new legal challenges, has a lot of lessons for AI law. "Some of those lessons are readily applicable to robots for example, the idea that architecture or \'code\' can be a kind of regulatory force, or that disciplines like computer science and law should talk to each other," Calo said.<br><br>But other lessons don\'t translate, especially when it\'s not just information that can be damaged. "Courts won\'t be so comfortable when bones instead of bits are on the line. I call this the problem of embodiment," he said.<br><br>"We may need a new model entirely. We may need a Federal Robotics Commission to help other agencies, courts, and state and federal lawmakers understand the technology well enough to make policy."<br><br>Such a move would ensure that AI and robotics get the attention that they need as a new technology, while still hewing to familiar legislative approaches.<br><br>Boston Dynamics\' "Petman" robot. Petman is ostensibly being developed to test military clothing and other equipment. Google acquired Boston Dynamics in 2013.<strong>Make law, not war</strong>There are less sanguine lessons for places where ethics have always been harder to enforce, though. In March 2015, the US Army sponsored a workshop that imagined what the battlefield will look like in 2050. Among its conclusions, it saw a huge increase in the role of artificial intelligence, not just in processing data but prosecuting warfare, putting the human soldiers "on the loop" rather than in it.<br><br>The workshop also predicted automated decision making, misinformation as a weapon, micro-targeting, large-scale self-organisation, and swarms of robots that would act independently or collaboratively. Even with humans in control, modern warfare is exceptionally prone to civilian collateral damage. With machines calling the shots in an environment filled with automated deception, what happens?<br><br>With so much AI development happening through open-source collaboration Elon Musk and Sam Altman recently announced a billion-dollar investment in OpenAI, a research company devoted to keeping AI developments generally available one ethical decision is immediately important. If you are developing AI techniques, do you want them used in war? If not, how can that be stopped?<br><br><strong>Myth: AI will spin out of control</strong><br><br>It\'s hard not to notice when intellectual and business celebrities of the calibre of Stephen Hawking and Elon Musk characterise AI as enough of a threat to imperil the very existence of humanity.<br><br>"The development of full artificial intelligence could spell the end of the human race," Hawking has said. "Humans, who are limited by slow biological evolution, couldn\'t compete and would be superseded." Musk is equally cheerless, saying back in 2014 that strong AI is "potentially more dangerous than nukes, and more recently that AI is "our biggest existential threat."<br><br>According to these technological luminaries, a sufficiently capable AI will not only be able to outthink us humans, but will necessarily evolve its own motivations and plans while being able to disguise and protect them, and itself, from us. And then we\'ll be in trouble.<br><br>Exactly how this scenario will come about has not been made clear, though. The leading theorist and cheerleader for mankind\'s imminent disappearance into insignificance or worse is Ray Kurzweil, who extrapolates the exponential growth in technological capability characterised by Moore\'s law to a point in the mid 2040s the Singularity where AI will be self-perpetuating and no longer reliant on human intellect.<br><br>Counter-arguments are plentiful, not least from the observation that exponential growth is frequently limited by outside factors that become more important as that growth continues. Moore\'s law itself, which states that every couple of years or so the number of transistors on a given area of silicon will double, has held good for fifty years but is deeply tied to aspects of basic physics that place hard limits on its future.<br><br>As transistors get smaller they are capable of switching at higher speeds, but they also suffer from exponential increases in leakage due to quantum tunnelling. This is a complex subject, but in essence: as the various layers inside a transistor get thinner and thinner, it\'s easier for electrons to tunnel through. At the very least this tunnelling effect significantly increases power consumption, but it can potentially cause a catastrophic failure.<br><br>Moore\'s law is only one half of the problem. The clock speed of processors regularly doubled from the mid-70s to the mid noughties, when it ran into another problem: an unmanageable increase in electrical power required, plus the corollary requirement of keeping these mega-power-dense chips from frying themselves.<br><br>While chips have continued to shrink, the max power consumption of a high-end computer chip has mostly stayed put. The end result is that we\'re now trying to shift about 100 watts of thermal energy from a chip that might only be 10 millimetres on each side, which is rather difficult. We\'ll soon need a novel cooling solution to go any further, lest we butt up against some laws of thermodynamics.<br><br>Ultimately, the biggest limit is that transistors are made of atoms, and we\'re approaching the point where we can\'t make a transistor any smaller or remove more atoms and still have a working device. Industry roadmaps point to the mid-2020s at the latest, but even today we\'re starting to feel the squeeze of the laws of physics. Intel said this year that 2016\'s switch from 14 nanometre transistors where the smallest component is around 27 atoms across to 10 nanometreswas on hold, stretching Moore\'s two years to at least three.<br><br><strong>Waiting for the next big break</strong><br><br>For the time being, then, most efforts have been focussed on multiple cores, arguing that two cores at 2 GHz are as good as one at 4 GHz but for the most part they aren\'t, as relatively few computing tasks can be efficiently split up to run across multiple cores.<br><br>The other big change in the last few years has been the rampant growth of large, centralised computing installations in data centres, public and hybrid clouds, and supercomputers. Performance gains have been hard to come by at a micro scale, and so companies and institutions have been going macro, where efficiencies of processing data at scale can be realised.<br><br>Siri doesn\'t live on your iPhone: she lives in Apple\'s data centres; the Xbox One can\'t handle the physics of a destructible environment in real time, and so it\'s off-loaded to Microsoft Azure instead.<br><br>Even in the data centre or supercomputer, though, other factors limit expansion. Again, most notably, power consumption and heat dissipation, but also the speed of light. The speed of light, which governs just about every digital communications interconnect, from copper wires to optical fibre to Wi-Fi, sets a hard limit on how much information can flow into and out of computer chips for processing. It already impacts how some specialised AI, most notably real-time financial analysis and high-frequency trading, can work.<br><br>The Trading Mesh For example, three years ago a networking company built an above-ground microwave network between London and Frankfurt, halving the round-trip latency of the existing fibre network from 8.35ms to 4.6ms. The network was used in secret for high-frequency trading for a full year before it became public knowledge. It only cost about £10 million ($15 million) to build the network connection between the two cities, but the trader may have made a profit of hundreds of millions of pounds.<br><br>Nobody knows how strong AI will work, but it must involve processing vast amounts of information. Unless it gets smart enough to find an entire alternative set of physical laws that appear to be hard-coded into the structure of spacetime, it will always be limited by how fast it can compare information held in different places. Quantum physics itself is rapidly evolving the tools to consider information as being as fundamental to the functioning of the universe, and as circumscribed by law, as energy. These promise a real answer to how smart AI can get, long before it gets there.<br><br><strong>Myth: AI will be a series of sudden breakthroughs</strong><br><br>In his 1964 short story Dial F For Frankenstein, Arthur C. Clarke described all the phones in the world simultaneously sounding a single ring as the global telephone system achieved sentience. Clarke later claimed that Tim Berners-Lee acknowledged this as one inspiration behind the invention of the Web well, perhaps. But the image of a system "waking up" and becoming aware is central to many future mythologies of AI.<br><br>Reality seems disinclined to follow. The development and advancement of AI is happening in a slow and deliberate fashion. Only now, after some fifty years of development, is AI is starting to make inroads into advanced applications such as healthcare, education, and finance. And again, these are still very narrow applications; you won\'t find an AI financial adviser that can also help diagnose your rare tropical disease.<br><br>The myth of a "big bang" AI breakthrough has damaged the field many times in the past, with heightened expectations and associated investments leading to a wholesale withdrawal from research when predictions weren\'t met.<br><br>These "AI winters" have occurred around the world and on a regular basis. In the 1980s, the Japanese government funded a half-billion dollar "Fifth Generation" project designed to leapfrog Western technology through massively parallel supercomputers that effectively programmed themselves when presented with logically-defined problems. By the time the project finished, nothing commercially useful had been produced while Western computing systems outperformed them by evolving conventional techniques. Funding for AI stopped.<br><br>Much the same had happened in the UK in the early 1970s, where most government investment in AI was cancelled after the Lighthill Report to Parliament concluded that none of the promised benefits of AI showed any sign of being useful in the real world. The report criticized AI\'s "grandiose objectives" compared to its production of "toy" systems unable to cope with the complexities of actual data. Once again, the point was made that conventional approaches outperformed, and seemed likely to continue to outperform, anything that AI could realistically deliver.<br><br>Ironically, many failed AI projects machine translation in the early 1960s, initial neural networks in the later 1960s, speech recognition in the 1970s, "expert systems" that codified business knowledge in the 1980s have become realities through the development ofcloud computing that couples very large amounts of computation with very large data sets. This commercially driven infrastructure, built for prosaic business reasons rather than ostensible advancement of AI, argues for gradual development in sync with utility.<br><br>What it really boils down to, then, is money. Commercialism is a forcing factor: it pushes businesses to continually improve their products, to adapt and develop their AI software as they go along. Until, of course, they create an AI that can adapt and develop itself, without human intervention. But that\'s still a long way off. Probably.<br><br>Rupert Goodwins started out as an engineer working for Clive Sinclair, Alan Sugar, and some other 1980s startups. He is now a London-based technology journalist who\'s written and broadcast about the digital world for more than thirty years. You can follow him on Twitter at @rupertg.<br><br>This post originated on Ars Technica UK </p><button class = "btn btn-info" onClick="hideDebunkingAI()">Hide Source</button>';
  document.getElementById("divDebunkingAI").className = "col-lg-12 col-md-12 col-sm-12 col-xs-12";
}
function hideDebunkingAI() {
  document.getElementById("debunkingAI").innerHTML='<button class = "btn btn-primary" onClick="showDebunkingAI()">Show Source</button>';
  document.getElementById("divDebunkingAI").className = "col-lg-6 col-md-6 col-sm-12 col-xs-12";
}


function showSingularityBust() {
  document.getElementById("singularityBust").innerHTML='<iframe width="640" height="480" src="https://www.youtube.com/embed/owppju3jwPE" frameborder="0" allowfullscreen></iframe><button class = "btn btn-info" onClick="hideSingularityBust()">Hide Source</button>';
  document.getElementById("divSingularityBust").className = "col-lg-12 col-md-12 col-sm-12 col-xs-12";
}
function hideSingularityBust() {
  document.getElementById("singularityBust").innerHTML='<button class = "btn btn-primary" onClick="showSingularityBust()">Show Source</button>';
  document.getElementById("divSingularityBust").className = "col-lg-12 col-md-12 col-sm-12 col-xs-12";
}


